[{"content":"nginx 安装 常用版本4大阵营\nNginx开源版本：nginx Nginx plus 商业版：Welcome to F5 NGINX openresty：OpenResty® - 开源官方站 Tengine：The Tengine Web Server 安装参考链接：nginx：Linux 软件包\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # 前提条件 sudo apt install curl gnupg2 ca-certificates lsb-release debian-archive-keyring # 导入官方 nginx 签名密钥 curl https://nginx.org/keys/nginx_signing.key | gpg --dearmor \\ | sudo tee /usr/share/keyrings/nginx-archive-keyring.gpg \u0026gt;/dev/null # 验证下载的文件是否包含正确的密钥 root@debian:~# gpg --dry-run --quiet --no-keyring --import --import-options import-show /usr/share/keyrings/nginx-archive-keyring.gpg pub rsa4096 2024-05-29 [SC] 8540A6F18833A80E9C1653A42FD21310B49F6B46 uid nginx signing key \u0026lt;signing-key-2@nginx.com\u0026gt; pub rsa2048 2011-08-19 [SC] [expires: 2027-05-24] 573BFD6B3D8FBC641079A6ABABF5BD827BD9BF62 uid nginx signing key \u0026lt;signing-key@nginx.com\u0026gt; pub rsa4096 2024-05-29 [SC] 9E9BE90EACBCDE69FE9B204CBCDCD8A38D88A2B3 uid nginx signing key \u0026lt;signing-key-3@nginx.com\u0026gt; # 为稳定的 nginx 软件包设置 apt 存储库 echo \u0026#34;deb [signed-by=/usr/share/keyrings/nginx-archive-keyring.gpg] \\ http://nginx.org/packages/debian `lsb_release -cs` nginx\u0026#34; \\ | sudo tee /etc/apt/sources.list.d/nginx.list # 安装 nginx sudo apt update sudo apt install nginx nginx 启动及验证 1 2 3 4 5 6 7 8 # 启动 root@debian:~# /usr/sbin/nginx root@debian:~# ps -ef|grep nginx | grep -v grep root 6683 1 0 19:54 ? 00:00:00 nginx: master process /usr/sbin/nginx nginx 6684 6683 0 19:54 ? 00:00:00 nginx: worker process nginx 6685 6683 0 19:54 ? 00:00:00 nginx: worker process nginx 6686 6683 0 19:54 ? 00:00:00 nginx: worker process nginx 6687 6683 0 19:54 ? 00:00:00 nginx: worker process vmware 上配置本地端口到虚机端口的映射 虚机上如果有防火墙，注意防火墙的配置 配置启动服务 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 # 查看/lib/systemd/system/nginx.service 或 /usr/lib/systemd/system/nginx.service 文件是否存在 # 注意安装路径的修改 [Unit] Description=nginx - high performance web server Documentation=https://nginx.org/en/docs/ After=network-online.target remote-fs.target nss-lookup.target Wants=network-online.target [Service] Type=forking PIDFile=/run/nginx.pid Environment=\u0026#34;CONFFILE=/etc/nginx/nginx.conf\u0026#34; EnvironmentFile=-/etc/default/nginx ExecStart=/usr/sbin/nginx -c ${CONFFILE} ExecReload=/bin/sh -c \u0026#34;/bin/kill -s HUP $(/bin/cat /run/nginx.pid)\u0026#34; ExecStop=/bin/sh -c \u0026#34;/bin/kill -s TERM $(/bin/cat /run/nginx.pid)\u0026#34; ExecQuit=/bin/sh -c \u0026#34;/bin/kill -s QUIT $(/bin/cat /run/nginx.pid)\u0026#34; [Install] WantedBy=multi-user.target # 查看服务是否可用，开机自启动 systemctl daemon-reload systemctl status nginx systemctl enable nginx nginx 常用命令 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 # 查看版本 root@debian:~# nginx -v nginx version: nginx/1.28.0 # 停止命令 root@debian:~# nginx -s stop root@debian:~# ps -ef|grep nginx | grep -v grep # 启动命令 root@debian:~# /usr/sbin/nginx root@debian:~# ps -ef|grep nginx | grep -v grep root 6797 1 0 20:21 ? 00:00:00 nginx: master process /usr/sbin/nginx nginx 6798 6797 0 20:21 ? 00:00:00 nginx: worker process nginx 6799 6797 0 20:21 ? 00:00:00 nginx: worker process nginx 6800 6797 0 20:21 ? 00:00:00 nginx: worker process nginx 6801 6797 0 20:21 ? 00:00:00 nginx: worker process # 配置重新加载 root@debian:~# /usr/sbin/nginx -s reload root@debian:~# ps -ef|grep nginx | grep -v grep root 6797 1 0 20:21 ? 00:00:00 nginx: master process /usr/sbin/nginx nginx 6809 6797 0 20:23 ? 00:00:00 nginx: worker process nginx 6810 6797 0 20:23 ? 00:00:00 nginx: worker process nginx 6811 6797 0 20:23 ? 00:00:00 nginx: worker process nginx 6812 6797 0 20:23 ? 00:00:00 nginx: worker process # 配置默认路径 root@debian:~# ls -l /etc/nginx/ total 28 drwxr-xr-x 2 root root 4096 May 17 19:39 conf.d -rw-r--r-- 1 root root 1007 Apr 23 07:48 fastcgi_params -rw-r--r-- 1 root root 5349 Apr 23 07:48 mime.types lrwxrwxrwx 1 root root 22 Apr 23 08:39 modules -\u0026gt; /usr/lib/nginx/modules -rw-r--r-- 1 root root 644 Apr 23 08:39 nginx.conf -rw-r--r-- 1 root root 636 Apr 23 07:48 scgi_params -rw-r--r-- 1 root root 664 Apr 23 07:48 uwsgi_params 停止nginx的其他方式 参考链接：Beginner’s Guide\n1 2 3 4 5 6 7 8 # to stop nginx processes with waiting for the worker processes to finish serving current requests nginx -s quit # 通过 kill 命令 [ -f /var/run/nginx.pid ] \u0026amp;\u0026amp; cat /var/run/nginx.pid | xargs kill -s QUIT # 查看nginx资源使用情况 ps axw -o pid,ppid,user,%cpu,vsz,wchan,command | egrep \u0026#39;(nginx|PID)\u0026#39; 配置文件 指令种类：简单指令，块指令。\n全局块：就是最开始的简单指令。从配置文件开始到events\n1 2 3 4 5 user nginx; worker_processes auto; error_log /var/log/nginx/error.log notice; pid /run/nginx.pid; events块：配置服务器和用户网络连接相关的参数。\nhttp块：配置代理、缓存、日志及第三方模块等。\n最小化配置文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 # cat /etc/nginx/nginx.conf user nginx; # worker进程启动时以哪个用户启动，限制 worker 进程的权限，提高安全性。 worker_processes auto; # 启动nginx时，启动多少个worker进程，设置为 CPU 核心数或 2 倍 CPU 核心数。 error_log /var/log/nginx/error.log notice; # 指定错误日志的路径和日志级别。notice 表示记录 notice 级别及以上的日志（如 notice、warn、error）。 pid /run/nginx.pid; # 指定存储 Nginx 主进程 PID 的文件路径。 events { worker_connections 1024; # 设置每个 worker 进程的最大连接数。总并发连接数 = worker_processes × worker_connections。 } http { include /etc/nginx/mime.types; # 包含MIME类型配置文件 default_type application/octet-stream; # 默认的MIME类型 log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39; \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39; \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;; # 定义日志格式。main 是日志格式的名称，后面是具体的格式字符串。 access_log /var/log/nginx/access.log main; # 指定访问日志的路径和日志格式。 sendfile on; # 启用 sendfile 机制，直接通过内核发送文件，减少用户态和内核态之间的数据拷贝。 #tcp_nopush on; # 启用 TCP_NOPUSH 选项，确保数据包填满后再发送 keepalive_timeout 65; # 设置客户端保持连接的超时时间（单位为秒）。 #gzip on; # 启用 gzip 压缩 include /etc/nginx/conf.d/*.conf; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 # /etc/nginx/conf.d/default.conf # server 块可以是多个 server { listen 80; # 指定 Nginx 监听的端口号。 server_name localhost; # 指定服务器的域名或主机名。 #access_log /var/log/nginx/host.access.log main; # location 块可以是多个 location / { # 定义根路径（/）的请求处理规则。 root /usr/share/nginx/html; # 指定根路径的静态文件存储目录。 index index.html index.htm; # 指定默认的索引文件。 } #error_page 404 /404.html; # redirect server error pages to the static page /50x.html # error_page 500 502 503 504 /50x.html; # 定义当服务器返回 500、502、503 或 504 错误时，重定向到 /50x.html 页面。 location = /50x.html { # 定义 /50x.html 路径的请求处理规则。= 表示精确匹配。 root /usr/share/nginx/html; # 指定 /50x.html 文件的存储目录。 } # proxy the PHP scripts to Apache listening on 127.0.0.1:80 # #location ~ \\.php$ { # 匹配所有以 .php 结尾的请求路径。~ 表示正则表达式匹配。 # proxy_pass http://127.0.0.1; # 将 PHP 请求代理到 http://127.0.0.1（通常是 Apache 服务器）。 #} # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # #location ~ \\.php$ { # 匹配所有以 .php 结尾的请求路径。~ 表示正则表达式匹配。 # root html; # 指定根路径的静态文件存储目录。 # fastcgi_pass 127.0.0.1:9000; # 将 PHP 请求转发给 FastCGI 服务器（如 PHP-FPM）处理。 # fastcgi_index index.php; # 指定FastCGI 服务器默认的索引文件。 # fastcgi_param SCRIPT_FILENAME /scripts$fastcgi_script_name; # 设置 FastCGI 参数，指定 PHP 脚本的路径。 # include fastcgi_params; # 包含 FastCGI 的默认参数配置文件。 #} # deny access to .htaccess files, if Apache\u0026#39;s document root # concurs with nginx\u0026#39;s one # #location ~ /\\.ht { # 匹配所有以 .ht 开头的文件路径（如 .htaccess）。 # deny all; # 禁止访问匹配的文件。 #} } 反向代理 单台代理 目标 在浏览器访问一个地址: http://127.0.0.1:3000/。 Nginx接受上面的请求。 转发请求到tomcat。 tomcat响应一个页面，页面中有：\u0026ldquo;tomcat hello !!!\u0026quot;。 操作步骤1：安装tomcat 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 # 安装java sudo apt install openjdk-17-jdk # 安装tomcat root@debian:~# wget https://dlcdn.apache.org/tomcat/tomcat-11/v11.0.7/bin/apache-tomcat-11.0.7.tar.gz root@debian:~# sudo tar -xzf apache-tomcat-11.0.7.tar.gz -C /opt root@debian:~# sudo mv /opt/apache-tomcat-11.0.7/ /opt/tomcat # 配置环境变量 root@debian:/opt/tomcat# vim /etc/profile if [ -d /opt/tomcat ]; then export CATALINA_HOME=/opt/tomcat export PATH=$PATH:$CATALINA_HOME/bin fi # 创建tomcat用户 root@debian:/opt/tomcat# sudo useradd -r -m -U -d /opt/tomcat -s /bin/false tomcat useradd: warning: the home directory /opt/tomcat already exists. useradd: Not copying any file from skel directory into it. root@debian:/opt/tomcat# root@debian:/opt/tomcat# sudo chown -R tomcat: /opt/tomcat # 配置tomcat服务 root@debian:/opt/tomcat# vim /etc/systemd/system/tomcat.service [Unit] Description=Apache Tomcat Web Application Container After=network.target [Service] Type=forking User=tomcat Group=tomcat Environment=\u0026#34;JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64\u0026#34; Environment=\u0026#34;CATALINA_PID=/opt/tomcat/temp/tomcat.pid\u0026#34; Environment=\u0026#34;CATALINA_HOME=/opt/tomcat\u0026#34; Environment=\u0026#34;CATALINA_BASE=/opt/tomcat\u0026#34; ExecStart=/opt/tomcat/bin/startup.sh ExecStop=/opt/tomcat/bin/shutdown.sh RestartSec=10 Restart=always [Install] WantedBy=multi-user.target # 生成目标页面 root@debian:~ # cd /opt/tomcat/webapps/ROOT root@debian:/opt/tomcat/webapps/ROOT# vim index.html tomcat hello !!! # 启动服务 root@debian:/opt/tomcat# sudo systemctl daemon-reload root@debian:/opt/tomcat# sudo systemctl start tomcat root@debian:/opt/tomcat# sudo systemctl status tomcat ● tomcat.service - Apache Tomcat Web Application Container Loaded: loaded (/etc/systemd/system/tomcat.service; disabled; preset: enabled) Active: active (running) since Mon 2025-05-19 09:36:27 EDT; 5s ago Process: 2271 ExecStart=/opt/tomcat/bin/startup.sh (code=exited, status=0/SUCCESS) Main PID: 2278 (java) Tasks: 38 (limit: 2241) Memory: 80.5M CPU: 2.304s CGroup: /system.slice/tomcat.service └─2278 /usr/lib/jvm/java-17-openjdk-amd64/bin/java -Djava.util.logging.config.file=/opt/tomcat/conf/logging.properties -Djava.util.logging.m\u0026gt; May 19 09:36:27 debian systemd[1]: Starting tomcat.service - Apache Tomcat Web Application Container... May 19 09:36:27 debian startup.sh[2271]: Tomcat started. May 19 09:36:27 debian systemd[1]: Started tomcat.service - Apache Tomcat Web Application Container. # 本地验证 root@debian:/opt/tomcat/webapps/ROOT# curl localhost:8080/index.html tomcat hello !!! 操作步骤2：nginx配置 1 2 3 4 5 6 7 8 9 10 11 # 配置nginx代理地址:端口 root@debian:/opt/tomcat/webapps/ROOT# cd /etc/nginx/conf.d/ root@debian:/etc/nginx/conf.d# vim default.conf location / { # root /usr/share/nginx/html; # index index.html index.htm; proxy_pass http://127.0.0.1:8080; } # 重新加载nginx配置 root@debian:/etc/nginx/conf.d# /usr/sbin/nginx -s reload 操作步骤3：本地验证 1 2 C:\\Users\\YY\u0026gt;curl http://127.0.0.1:3000 tomcat hello !!! 多台代理 目标 浏览器访问：（http://127.0.0.1:3000/beijing），通过nginx，跳转到一个tomcat上 （http://localhost:8081），在浏览器上显示：beijing。 浏览器访问：（http://127.0.0.1:3000/shanghai），通过nginx，跳转到一个tomcat上（http://localhost:8082），在浏览器上显示：shanghai 操作步骤1：安装tomcat 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # 在单台代理的基础上，拷贝两个tomcat，再配置两个tomcat root@debian:/opt/tomcat8081/conf# vim server.xml \u0026lt;Server port=\u0026#34;8015\u0026#34; shutdown=\u0026#34;SHUTDOWN\u0026#34;\u0026gt; \u0026lt;Connector port=\u0026#34;8081\u0026#34; protocol=\u0026#34;HTTP/1.1\u0026#34; connectionTimeout=\u0026#34;20000\u0026#34; redirectPort=\u0026#34;8443\u0026#34; /\u0026gt; root@debian:/opt/tomcat8082/conf# vim server.xml \u0026lt;Server port=\u0026#34;8025\u0026#34; shutdown=\u0026#34;SHUTDOWN\u0026#34;\u0026gt; \u0026lt;Connector port=\u0026#34;8082\u0026#34; protocol=\u0026#34;HTTP/1.1\u0026#34; connectionTimeout=\u0026#34;20000\u0026#34; redirectPort=\u0026#34;8453\u0026#34; /\u0026gt; # 停止tomcat root@debian:/opt/tomcat8082/conf# systemctl stop tomcat root@debian:/opt/tomcat8082/conf# systemctl status tomcat ○ tomcat.service - Apache Tomcat Web Application Container Loaded: loaded (/etc/systemd/system/tomcat.service; disabled; preset: enabled) Active: inactive (dead) # 修改html文件内容 root@debian:/opt/tomcat8081/webapps/beijing # vim index.html beijing root@debian:/opt/tomcat8082/webapps/shanghai # vim index.html shanghai # 启动服务并验证 export CATALINA_HOME=/opt/tomcat8081 export PATH=$CATALINA_HOME/bin:$PATH root@debian:/opt/tomcat8081# ./bin/startup.sh root@debian:/opt/tomcat8081# curl http://localhost:8081 beijing export CATALINA_HOME=/opt/tomcat8082 export PATH=$CATALINA_HOME/bin:$PATH root@debian:/opt/tomcat8082# ./bin/startup.sh root@debian:/opt/tomcat8082# curl http://localhost:8082 shanghai 操作步骤2：nginx配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 配置nginx代理地址:端口 root@debian:/opt/tomcat/webapps/ROOT# cd /etc/nginx/conf.d/ root@debian:/etc/nginx/conf.d# vim default.conf location ~ /beijing/ { proxy_pass http://127.0.0.1:8081; } location ~ /shanghai/ { proxy_pass http://127.0.0.1:8082; } location / { # root /usr/share/nginx/html; # index index.html index.htm; proxy_pass http://127.0.0.1:8080; } # 重新加载nginx配置 root@debian:/etc/nginx/conf.d# /usr/sbin/nginx -s reload 操作步骤3：本地验证 1 2 3 4 5 6 7 8 9 10 11 C:\\Users\\YY\u0026gt;curl http://127.0.0.1:3000/shanghai/ shanghai C:\\Users\\YY\u0026gt;curl http://127.0.0.1:3000/shanghai/index.html shanghai C:\\Users\\YY\u0026gt;curl http://127.0.0.1:3000/beijing/index.html beijing C:\\Users\\YY\u0026gt;curl http://127.0.0.1:3000/beijing/ beijing 负载均衡 目标 通过浏览器多次访问一个地址（http://www.cainiao.com/load-balance）。 nginx接受上面的请求，并进行转发。 那么每个请求的响应，是来自于不同的tomcat提供的。（2台tomcat，端口：8081，8082）。 两台tomcat，不同的响应内容：“8081”和“8082”。 操作步骤 部署tomcat 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 还是使用多台代理时的tomcat root@debian:~# cd /opt/tomcat8081/webapps root@debian:/opt/tomcat8081/webapps# mkdir load-balance root@debian:/opt/tomcat8081/webapps# vim load-balance/index.html 8081 root@debian:~# cd /opt/tomcat8082/webapps root@debian:/opt/tomcat8082/webapps# mkdir load-balance root@debian:/opt/tomcat8082/webapps# vim load-balance/index.html 8082 # 启动服务 export CATALINA_HOME=/opt/tomcat8081 export PATH=$CATALINA_HOME/bin:$PATH root@debian:/opt/tomcat8081# /opt/tomcat8081/bin/startup.sh export CATALINA_HOME=/opt/tomcat8082 export PATH=$CATALINA_HOME/bin:$PATH root@debian:/opt/tomcat8082# /opt/tomcat8082/bin/startup.sh root@debian:/opt/tomcat8082# curl http://127.0.0.1/load-balance/index.html 8081 root@debian:/opt/tomcat8082# curl http://127.0.0.1/load-balance/index.html 8082 配置nginx 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 root@debian:/opt/tomcat8082# cat /etc/nginx/conf.d/default.conf # upstream 要配置在http块中 upstream myServers { server 127.0.0.1:8081; # server 指令只能指定 IP 地址（或域名）和端口 server 127.0.0.1:8082; } server { listen 80; server_name localhost; location / { # root /usr/share/nginx/html; # index index.html index.htm; proxy_pass http://myServers; } ...... } # 重新加载服务配置 root@debian:/etc/nginx/conf.d# /usr/sbin/nginx -s reload 注意\nupstream 要配置在http块中 upstream 的server 指令只能指定 IP 地址（或域名）和端口 验证 1 2 3 4 5 C:\\Users\\YY\u0026gt;curl http://127.0.0.1:3000/load-balance/index.html 8081 C:\\Users\\YY\u0026gt;curl http://127.0.0.1:3000/load-balance/index.html 8082 负载均衡的方法（算法） 参考链接：https://docs.nginx.com/nginx/admin-guide/load-balancer/http-load-balancer/\nRound Robin Requests are distributed evenly across the servers, with server weights taken into consideration. This method is used by default (there is no directive for enabling it)\n1 2 3 4 5 upstream backend { # no load balancing method is specified for Round Robin server backend1.example.com; server backend2.example.com; } Least Connections A request is sent to the server with the least number of active connections, again with server weights taken into consideration\n1 2 3 4 5 upstream backend { least_conn; server backend1.example.com; server backend2.example.com; } IP Hash The server to which a request is sent is determined from the client IP address. In this case, either the first three octets of the IPv4 address or the whole IPv6 address are used to calculate the hash value. The method guarantees that requests from the same address get to the same server unless it is not available.\n通过这种方式，客户端的请求会转发到同一台服务器上\n1 2 3 4 5 6 upstream backend { ip_hash; server backend1.example.com; server backend2.example.com; server backend3.example.com down; # 如果一台服务器不能整体提供服务，自动转发到下一台服务器 } Generic Hash The server to which a request is sent is determined from a user‑defined key which can be a text string, variable, or a combination. For example, the key may be a paired source IP address and port, or a URI as in this example:\n1 2 3 4 5 upstream backend { hash $request_uri consistent; server backend1.example.com; server backend2.example.com; } Least Time (NGINX Plus only) For each request, NGINX Plus selects the server with the lowest average latency and the lowest number of active connections, where the lowest average latency is calculated based on which of the following parameters to the least_time directive is included:\n暂不涉及，使用时再查看文档\nRandom Each request will be passed to a randomly selected server. If the two parameter is specified, first, NGINX randomly selects two servers taking into account server weights, and then chooses one of these servers using the specified method:\nleast_conn – The least number of active connections least_time=header (NGINX Plus) – The least average time to receive the response header from the server ($upstream_header_time) least_time=last_byte (NGINX Plus) – The least average time to receive the full response from the server ($upstream_response_time) 1 2 3 4 5 6 7 upstream backend { random two least_time=last_byte; server backend1.example.com; server backend2.example.com; server backend3.example.com; server backend4.example.com; } Server Weights(权重) By default, NGINX distributes requests among the servers in the group according to their weights using the Round Robin method. The weight parameter to the server directive sets the weight of a server; the default is 1:\n1 2 3 4 5 6 upstream backend { server backend1.example.com weight=5; server backend2.example.com; server 192.0.0.1 backup; } # With this configuration of weights, out of every 6 requests, 5 are sent to backend1.example.com and 1 to backend2.example.com. 动静分离 什么是 Nginx 的动静分离？ 动静分离是指将 静态资源（如 HTML、CSS、JavaScript、图片等） 和 动态资源（如 PHP、Java、Python 等生成的动态内容） 分别处理，以提高服务器性能和资源利用率。\n静态资源：内容固定不变，可以直接从磁盘或缓存中读取，不需要经过后端应用服务器处理。 动态资源：内容根据请求动态生成，需要经过后端应用服务器（如 Tomcat、PHP-FPM 等）处理。 通过动静分离，可以将静态资源交给 Nginx 直接处理，而动态资源则转发给后端应用服务器处理，从而减轻后端服务器的压力，并提高静态资源的访问速度。\n为什么要做动静分离？ 提高性能：Nginx 处理静态资源的性能非常高，远高于后端应用服务器。 减轻后端压力：将静态资源交给 Nginx 处理，减少后端服务器的负载。 提高用户体验：静态资源加载更快，提升页面响应速度。 便于扩展：静态资源可以单独部署到 CDN 或专门的静态资源服务器上。 如何实现 Nginx 的动静分离？ 静态资源和动态资源的分类 静态资源：.html, .css, .js, .jpg, .png, .gif, .ico, .woff, .ttf 等。 动态资源：.php, .jsp, .do, .py 等。 Nginx 配置动静分离 通过 location 块将静态资源和动态资源分别处理。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 server { listen 8000; server_name cainiao.com; # 静态资源处理 location ~* \\.(html|css|js|jpg|jpeg|png|gif|ico|woff|ttf)$ { root /var/www/static; # 静态资源存放目录 expires 30d; # 设置缓存过期时间 } # 动态资源处理 location / { proxy_pass http://backend_server; # 转发到后端应用服务器 proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } upstream backend_server { server 127.0.0.1:8080; # 后端应用服务器地址 } 配置说明 静态资源处理： 使用 location ~* \\.(html|css|js|jpg|jpeg|png|gif|ico|woff|ttf)$ 匹配静态资源文件。 root /var/www/static：指定静态资源的存放目录。 expires 30d：设置静态资源的缓存过期时间，减少客户端请求。 动态资源处理： 使用 location / 匹配所有请求。 proxy_pass http://backend_server：将动态资源请求转发到后端应用服务器。 后端服务器： 使用 upstream 定义后端应用服务器组。 测试动静分离 配置修改步骤：\n1、域名映射\n1 2 C:\\Windows\\System32\\drivers\\etc\\hosts 127.0.0.1 cainiao.com 2、配置nginx\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 root@debian:/etc/nginx# vim nginx.conf # 在http模块的最后添加如下内容 server { listen 8000; server_name cainiao.com; # 静态资源处理 location ~* \\.(html|css|js|jpg|jpeg|png|gif|ico|woff|ttf)$ { root /var/www/static; # 静态资源存放目录 expires 30d; # 设置缓存过期时间 } # 动态资源处理 location / { proxy_pass http://backend_server; # 转发到后端应用服务器 proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; } } upstream backend_server { server 127.0.0.1:8080; # 后端应用服务器地址 } # 重新加载服务配置 root@debian:/etc/nginx/conf.d# /usr/sbin/nginx -s reload 3、配置资源\n1 2 root@debian:/etc/nginx# mkdir -p /var/www/static # 上传一个名为logo.png的图片到该目录下 4、主机配置端口映射\n5、浏览器验证静态资源\n6、浏览器验证动态资源\n进一步优化 使用 CDN： 将静态资源部署到 CDN（内容分发网络），进一步提高静态资源的访问速度。\n启用 Gzip 压缩： 在 Nginx 中启用 Gzip 压缩，减少静态资源的传输大小。\n1 2 gzip on; gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript; 设置缓存策略： 为静态资源设置合理的缓存策略，减少客户端请求。\n1 2 3 4 5 location ~* \\.(html|css|js|jpg|jpeg|png|gif|ico|woff|ttf)$ { root /var/www/static; expires 30d; add_header Cache-Control \u0026#34;public, no-transform\u0026#34;; } 高可用 架构图 环境准备 前提条件 拷贝复制前面nginx虚机，修改虚机IP为192.168.52.70\n安装keepalived 每个虚机都安装，详细可以参考：keepalived入门介绍\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 apt -y install keepalived # 主节点 root@debian:/etc/keepalived# vim keepalived.conf ! Configuration File for keepalived global_defs { } vrrp_instance VI_1 { state MASTER interface ens33 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.52.99 } } # 备节点 root@debian:/etc/keepalived# cat keepalived.conf ! Configuration File for keepalived global_defs { } vrrp_instance VI_1 { state BACKUP interface ens33 virtual_router_id 51 priority 80 advert_int 1 authentication { auth_type PASS auth_pass 1111 } virtual_ipaddress { 192.168.52.99 } } # 启动服务 root@debian:/etc/keepalived# systemctl start keepalived.service root@debian:/etc/keepalived# systemctl status keepalived.service ● keepalived.service - Keepalive Daemon (LVS and VRRP) Loaded: loaded (/lib/systemd/system/keepalived.service; enabled; preset: enabled) Active: active (running) since Tue 2025-05-20 22:14:30 EDT; 1s ago Docs: man:keepalived(8) man:keepalived.conf(5) man:genhash(1) https://keepalived.org Main PID: 1681 (keepalived) Tasks: 2 (limit: 2241) Memory: 3.7M CPU: 17ms CGroup: /system.slice/keepalived.service ├─1681 /usr/sbin/keepalived --dont-fork └─1682 /usr/sbin/keepalived --dont-fork 验证 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 # 停止主节点192.168.52.60上的keepalived服务 root@debian:~# ip a s 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host noprefixroute valid_lft forever preferred_lft forever 2: ens33: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 00:0c:29:dc:2d:71 brd ff:ff:ff:ff:ff:ff altname enp2s1 inet 192.168.52.60/24 brd 192.168.52.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet 192.168.52.99/32 scope global ens33 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fedc:2d71/64 scope link noprefixroute valid_lft forever preferred_lft forever root@debian:~# systemctl stop keepalived.service root@debian:~# ip a s 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host noprefixroute valid_lft forever preferred_lft forever 2: ens33: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 00:0c:29:dc:2d:71 brd ff:ff:ff:ff:ff:ff altname enp2s1 inet 192.168.52.60/24 brd 192.168.52.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fedc:2d71/64 scope link noprefixroute valid_lft forever preferred_lft forever 再次通过浏览器访问\n","date":"2024-07-24T00:00:00Z","image":"https://caijemmy.github.io/p/1.0-nginx/cover_hu_183eb973ddfe0d00.jpg","permalink":"https://caijemmy.github.io/p/1.0-nginx/","title":"1.0 nginx"},{"content":"servername的多种匹配方式 同一端口配置不同域名 提前准备 本地主机配置hosts文件\n1 2 3 4 C:\\Windows\\System32\\drivers\\etc\\hosts # nginx验证同一端口配置不同域名 192.168.52.60 cainiao1.com 192.168.52.60 cainiao2.com 虚拟主机做以下配置\n1 2 3 root@debian:/var/www# mkdir cainiao1 cainiao2 root@debian:/var/www# echo \u0026#39;this is \u0026lt;br\u0026gt; cainiao1\u0026#39; \u0026gt; cainiao1/index.html root@debian:/var/www# echo \u0026#39;this is \u0026lt;br\u0026gt; cainiao2\u0026#39; \u0026gt; cainiao2/index.html 配置nginx 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # 将以下内容防止配置文件http模块最后 root@debian:~# vim /etc/nginx/nginx.conf server { listen 80; server_name cainiao1.com; location / { root /var/www/cainiao1; index index.html index.htm; expires 30d; } } server { listen 80; server_name cainiao2.com; location / { root /var/www/cainiao2; index index.html index.htm; expires 30d; } } # 重新加载配置 root@debian:~# systemctl reload nginx.service 验证 不同域名指向同一访问路径 提前准备 本地主机配置hosts文件\n1 2 3 4 5 6 7 8 9 C:\\Windows\\System32\\drivers\\etc\\hosts # nginx验证同一端口配置不同域名 192.168.52.60 cainiao1.com 192.168.52.60 cainiao2.com 192.168.52.60 xxoo.cainiao1.com 192.168.52.60 xx.cainiao1.com 192.168.52.60 oo.cainiao1.com 192.168.52.60 cainiao2.org 192.168.52.60 cainiao2.net 配置nginx 注意：server_name配置域名时，域名间有空格\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 root@debian:~# vim /etc/nginx/nginx.conf server { listen 80; # server_name cainiao1.com xxoo.cainiao1.com xx.cainiao1.com oo.cainiao1.com; server_name cainiao1.com *.cainiao1.com; # 可以使用*通配符 location / { root /var/www/cainiao1; index index.html index.htm; expires 30d; } } server { listen 80; server_name cainiao2.*; location / { root /var/www/cainiao2; expires 30d; } } # 重新加载配置 root@debian:~# systemctl reload nginx.service 验证 负载均衡参数详解 nginx配置 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 root@debian:/opt/tomcat8082# cat /etc/nginx/conf.d/default.conf upstream myServers { server 127.0.0.1:8081 weight=4 down; server 127.0.0.1:8082 weight=1 backup; server 127.0.0.1:8083 weight=1 max_fails=3 fail_timeout=10s slow_start=30s; } server { listen 80; server_name localhost; location / { # root /usr/share/nginx/html; # index index.html index.htm; proxy_pass http://myServers; } ...... } 配置说明\nweight 在实际生产中使用普遍，主要根据服务器配置、网络带宽对集群权重进行调整。权重越高，分配的请求越多。 down 标记某个后端服务器为不可用状态，nginx不转发请求到此服务器。实际中几乎不用。 backup 用于将某台服务器标记为备用服务器。只有当所有非备用服务器不可用时，才会将请求分配到备用服务器。 max_fails 和 fail_timeout 在 fail_timeout 时间窗口内允许的最大失败次数（max_fails）。 resolve 动态解析域名，适用于后端服务器使用域名的情况。 slow_start 当服务器从 down 状态恢复时，逐渐增加其权重。 顺序说明\n虽然 Nginx 的 upstream 参数顺序不影响功能，但为了代码的规范性和可读性，建议按照以下顺序书写参数：\n服务器地址（IP 或域名） weight max_fails 和 fail_timeout backup down 其他参数（如 resolve、slow_start 等） 这样可以确保配置清晰易懂，便于后续维护和调试。\n负载均衡算法 实际配置情况\nRound Robin（轮询） 生产中使用这种，生产系统使用无状态会话技术即可。\nLeast Connections（最少连接） 生产中不用，后端服务无法会话保持。\nIP Hash 生产中不用，移动端IP变化后，后端服务无法会话保持。\nGeneric Hash url hash 定向流量转发，固定资源不在统一服务器，才用的到。可能会出现流量偏移。\nRandom 生产中不用，后端服务无法会话保持。\nNginx 的正则表达式 Nginx 的正则表达式（Regular Expressions）主要用于 location 块、rewrite 指令、if 条件等场景中，用于匹配 URL 或其他字符串。Nginx 使用的是 PCRE（Perl Compatible Regular Expressions） 库，因此其语法与 Perl 的正则表达式兼容。\n正则表达式的基本语法 Nginx 的正则表达式语法与 PCRE 一致，以下是一些常用的元字符和语法：\n元字符 描述 . 匹配任意单个字符（除换行符 \\n 外） * 匹配前面的字符 0 次或多次 + 匹配前面的字符 1 次或多次 ? 匹配前面的字符 0 次或 1 次 {n} 匹配前面的字符恰好 n 次 {n,} 匹配前面的字符至少 n 次 {n,m} 匹配前面的字符至少 n 次，至多 m 次 ^ 匹配字符串的开头 $ 匹配字符串的结尾 \\ 转义字符，用于匹配特殊字符（如 .、* 等） [] 字符集，匹配其中任意一个字符（如 [abc] 匹配 a、b 或 c） [^] 否定字符集，匹配不在其中的任意字符（如 [^abc] 匹配非 a、b、c 的字符） ` ` () 分组，用于捕获匹配的内容或定义子表达式 Nginx 中的正则表达式匹配 在 Nginx 中，正则表达式通常用于以下场景：\nlocation 块中的正则匹配 location 块可以使用正则表达式匹配 URL。正则表达式必须以 ~（区分大小写）或 ~*（不区分大小写）开头。\n区分大小写匹配：\n1 2 3 location ~ \\.php$ { # 匹配以 .php 结尾的 URL } 不区分大小写匹配：\n1 2 3 location ~* \\.(jpg|jpeg|png|gif)$ { # 匹配以 .jpg、.jpeg、.png 或 .gif 结尾的 URL，不区分大小写 } rewrite 指令中的正则匹配 rewrite 指令可以使用正则表达式匹配 URL 并进行重写。\n示例：\n1 rewrite ^/old/(.*)$ /new/\\$1 permanent; 解释：\n^/old/(.*)$：匹配以 /old/ 开头的 URL，并捕获后面的内容。 /new/\\$1：将匹配的 URL 重写为 /new/ 加上捕获的内容。 permanent：返回 301 永久重定向。 if 条件中的正则匹配 if 指令可以使用正则表达式进行条件判断。\n示例：\n1 2 3 if ($request_uri ~* \u0026#34;^/admin\u0026#34;) { return 403; } 解释：\n如果请求的 URL 以 /admin 开头（不区分大小写），则返回 403 状态码。 正则表达式的捕获与引用 在 Nginx 中，正则表达式的捕获内容可以通过 \\$1、\\$2 等变量引用。\n示例：\n1 2 3 location ~ ^/user/(\\d+)$ { proxy_pass http://backend/user/\\$1; } 解释：\n(\\d+)：捕获 URL 中的数字部分。 \\$1：引用捕获的内容。 综合示例 以下是一个综合示例，展示了正则表达式在 Nginx 中的使用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 server { listen 80; server_name example.com; # 匹配以 /user/ 开头的 URL，并捕获用户 ID location ~ ^/user/(\\d+)$ { proxy_pass http://backend/user/\\$1; } # 匹配以 .jpg、.jpeg、.png 或 .gif 结尾的 URL，不区分大小写 location ~* \\.(jpg|jpeg|png|gif)$ { access_log off; expires 30d; } # 重写 /old/ 开头的 URL 到 /new/ rewrite ^/old/(.*)$ /new/\\$1 permanent; # 如果请求的 URL 以 /admin 开头，返回 403 if ($request_uri ~* \u0026#34;^/admin\u0026#34;) { return 403; } } ","date":"2024-07-24T00:00:00Z","image":"https://caijemmy.github.io/p/2.0-nginx/cover_hu_183eb973ddfe0d00.jpg","permalink":"https://caijemmy.github.io/p/2.0-nginx/","title":"2.0 nginx"},{"content":"Kubernetes集群核心概念 Pod 工作负载(workloads) 参考链接：工作负载 | Kubernetes\n工作负载（workload）是在kubernetes集群中运行的应用程序。无论你的工作负载是单一服务还是多个一同工作的服务构成，在kubernetes中都可以使用pod来运行它。\nworkloads分为pod与controllers\npod通过控制器实现应用的运行，如何伸缩，升级等 controllers 在集群中管理pod pod与控制器之间通过label-selector相关联，是唯一的关联方式 在pod的YAML里指定pod标签\n1 2 3 # 定义标签 labels: app: nginx 在控制器的YAML里指定标签选择器匹配标签\n1 2 3 4 # 通过标签选择器选择对应的pod selector: matchLabels: app: nginx pod介绍 参考链接: Pod | Kubernetes\n查看pod方法 pod是一种计算资源，可以通过kubectl get pod来查看\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 [root@k8s-master01 ~]# kubectl get pod\t# pod或pods都可以，不指定namespace,默认是名为default的namespace No resources found in default namespace. [root@k8s-master01 ~]# kubectl get pod -n kube-system NAME READY STATUS RESTARTS AGE coredns-5dd5756b68-gbgsh 1/1 Running 13 (17h ago) 4d4h coredns-5dd5756b68-pm85d 1/1 Running 13 (17h ago) 4d4h etcd-k8s-master01 1/1 Running 13 (17h ago) 4d4h kube-apiserver-k8s-master01 1/1 Running 14 (17h ago) 4d4h kube-controller-manager-k8s-master01 1/1 Running 13 (17h ago) 4d4h kube-proxy-h7s9b 1/1 Running 7 (17h ago) 3d20h kube-proxy-qt8px 1/1 Running 7 (17h ago) 3d20h kube-proxy-wlvrg 1/1 Running 7 (17h ago) 3d20h kube-scheduler-k8s-master01 1/1 Running 13 (17h ago) 4d4h metrics-server-596474b58-697f8 1/1 Running 1 (17h ago) 19h [root@k8s-master01 ~]# pod的YAML资源清单格式 YAML格式查找帮助方法\n1 2 3 4 5 [root@k8s-master01 ~]# kubectl explain namespace [root@k8s-master01 ~]# kubectl explain pod [root@k8s-master01 ~]# kubectl explain pod.spec [root@k8s-master01 ~]# kubectl explain pod.spec.containers 样例，及供参考。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 # yaml格式的pod定义文件完整内容： apiVersion: v1 #必选，api版本号，例如v1 kind: Pod #必选，Pod metadata: #必选，元数据 name: string #必选，Pod名称 namespace: string #Pod所属的命名空间,默认在default的namespace labels: # 自定义标签 name: string #自定义标签名字 annotations: #自定义注释列表 name: string spec: #必选，Pod中容器的详细定义(期望) containers: #必选，Pod中容器列表 - name: string #必选，容器名称 image: string #必选，容器的镜像名称 imagePullPolicy: [Always | Never | IfNotPresent] #获取镜像的策略 Alawys表示下载镜像 IfnotPresent表示优先使用本地镜像，否则下载镜像，Nerver表示仅使用本地镜像 command: [string] #容器的启动命令列表，如不指定，使用打包时使用的启动命令 args: [string] #容器的启动命令参数列表 workingDir: string #容器的工作目录 volumeMounts: #挂载到容器内部的存储卷配置 - name: string #引用pod定义的共享存储卷的名称，需用volumes[]部分定义的的卷名 mountPath: string #存储卷在容器内mount的绝对路径，应少于512字符 readOnly: boolean #是否为只读模式 ports: #需要暴露的端口库号列表 - name: string #端口号名称 containerPort: int #容器需要监听的端口号 hostPort: int #容器所在主机需要监听的端口号，默认与Container相同 protocol: string #端口协议，支持TCP和UDP，默认TCP env: #容器运行前需设置的环境变量列表 - name: string #环境变量名称 value: string #环境变量的值 resources: #资源限制和请求的设置 limits: #资源限制的设置 cpu: string #Cpu的限制，单位为core数，将用于docker run --cpu-shares参数 memory: string #内存限制，单位可以为Mib/Gib，将用于docker run --memory参数 requests: #资源请求的设置 cpu: string #Cpu请求，容器启动的初始可用数量 memory: string #内存清求，容器启动的初始可用数量 livenessProbe: #对Pod内个容器健康检查的设置，当探测无响应几次后将自动重启该容器，检查方法有exec、httpGet和tcpSocket，对一个容器只需设置其中一种方法即可 exec: #对Pod容器内检查方式设置为exec方式 command: [string] #exec方式需要制定的命令或脚本 httpGet: #对Pod内个容器健康检查方法设置为HttpGet，需要制定Path、port path: string port: number host: string scheme: string HttpHeaders: - name: string value: string tcpSocket: #对Pod内个容器健康检查方式设置为tcpSocket方式 port: number initialDelaySeconds: 0 #容器启动完成后首次探测的时间，单位为秒 timeoutSeconds: 0 #对容器健康检查探测等待响应的超时时间，单位秒，默认1秒 periodSeconds: 0 #对容器监控检查的定期探测时间设置，单位秒，默认10秒一次 successThreshold: 0 failureThreshold: 0 securityContext: privileged:false restartPolicy: [Always | Never | OnFailure] # Pod的重启策略，Always表示一旦不管以何种方式终止运行，kubelet都将重启，OnFailure表示只有Pod以非0退出码退出才重启，Nerver表示不再重启该Pod nodeSelector: obeject # 设置NodeSelector表示将该Pod调度到包含这个label的node上，以key：value的格式指定 imagePullSecrets: #Pull镜像时使用的secret名称，以key：secretkey格式指定 - name: string hostNetwork: false #是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络 volumes: #在该pod上定义共享存储卷列表 - name: string #共享存储卷名称 （volumes类型有很多种） emptyDir: {} #类型为emtyDir的存储卷，与Pod同生命周期的一个临时目录。为空值 hostPath: string #类型为hostPath的存储卷，表示挂载Pod所在宿主机的目录 path: string #Pod所在宿主机的目录，将被用于同期中mount的目录 secret: #类型为secret的存储卷，挂载集群与定义的secret对象到容器内部 scretname: string items: - key: string path: string configMap: #类型为configMap的存储卷，挂载预定义的configMap对象到容器内部 name: string items: - key: string path: string pod创建与验证 命令创建pod(v1.18变化) k8s之前版本中, kubectl run命令用于创建deployment控制器 在v1.18版本中, kubectl run命令改为创建pod 创建一个名为pod-nginx的pod 1 2 [root@k8s-master01 ~]# kubectl run nginx1 --image=nginx pod/nginx1 created 验证 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 [root@k8s-master01 ~]# kubectl get pods NAME READY STATUS RESTARTS AGE nginx1 1/1 Running 0 41s # NAME：Pod 的名称。名称在同一命名空间内必须唯一。 # READY：Pod 中容器的就绪状态。格式为 就绪容器数量/总容器数量。 # STATUS：Pod 的当前状态。 # 常见的状态包括： # Running：Pod 中的所有容器已成功启动并正在运行。 # Pending：Pod 已被调度到节点，但容器尚未启动（例如，正在拉取镜像）。 # Succeeded：Pod 中的所有容器已成功完成任务并退出。 # Failed：Pod 中的至少一个容器因错误退出。 # CrashLoopBackOff：容器启动失败，Kubernetes 正在重试。 # Unknown：无法获取 Pod 的状态（通常是由于与节点通信失败）。 # RESTARTS：Pod 中容器的重启次数。 # AGE：Pod 的存活时间。从 Pod 创建到当前时间的时间间隔。时间单位可以是秒（s）、分钟（m）、小时（h）或天（d）。 YAML创建pod 准备yaml文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 [root@k8s-master01 ~]# vim pod1.yml apiVersion: v1\t# api版本 kind: Pod\t# 资源类型为Pod metadata:\tname: pod-stress\t# 自定义pod的名称 spec: containers:\t# 定义pod里包含的容器 - name: c1\t# 自定义pod中的容器名 image: polinux/stress\t# 启动容器的镜像名 command: [\u0026#34;stress\u0026#34;]\t# 自定义启动容器时要执行的命令(类似dockerfile里的CMD) args: [\u0026#34;--vm\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;--vm-bytes\u0026#34;, \u0026#34;150M\u0026#34;, \u0026#34;--vm-hang\u0026#34;, \u0026#34;1\u0026#34;] # 自定义启动容器执行命令的参数 # polinux/stress这个镜像用于压力测试,在启动容器时传命令与参数就是相当于分配容器运行时需要的压力 [root@k8s-master01 ~]# kubectl apply -f pod1.yml pod/pod-stress created 查看pod信息 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 [root@k8s-master01 ~/dashboard]# kubectl get pod NAME READY STATUS RESTARTS AGE pod-stress 1/1 Running 0 3m32s [root@k8s-master01 ~/dashboard]# kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES pod-stress 1/1 Running 0 4m14s 10.244.69.215 k8s-worker02 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; [root@k8s-master01 ~/dashboard]# kubectl describe pod pod-stress Name: pod-stress Namespace: default ...... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 4m50s default-scheduler Successfully assigned default/pod-stress to k8s-worker02 Normal Pulling 4m49s kubelet Pulling image \u0026#34;polinux/stress\u0026#34; Normal Pulled 4m38s kubelet Successfully pulled image \u0026#34;polinux/stress\u0026#34; in 11.549s (11.549s including waiting) Normal Created 4m38s kubelet Created container c1 Normal Started 4m38s kubelet Started container c1 删除pod 单个pod删除 1 2 3 4 5 6 # 方式1 [root@k8s-master01 ~/dashboard]# kubectl delete -f pod1.yml pod \u0026#34;pod-stress\u0026#34; deleted # 方式2 [root@k8s-master01 ~/dashboard]# kubectl delete pod pod-stress 多个pod删除 1 2 3 4 5 6 7 8 # 方法1: 后接多个pod名 [root@k8s-master1 ~]# kubectl delete pod pod名1 pod名2 pod名3 ...... # 方法2: 通过awk截取要删除的pod名称，然后管道给xargs [root@k8s-master1 ~]# kubectl get pods |awk \u0026#39;NR\u0026gt;1 {print $1}\u0026#39; |xargs kubectl delete pod # 方法3: 如果要删除的pod都在同一个非default的命名空间，则可直接删除命名空间 [root@k8s-master1 ~]# kubectl delete ns xxxx 镜像拉取策略 由imagePullPolicy参数控制\nAlways : 不管本地有没有镜像，都要从仓库中下载镜像 Never : 从来不从仓库下载镜像, 只用本地镜像,本地没有就算了 IfNotPresent: 如果本地存在就直接使用, 不存在才从仓库下载 默认的策略是：\n当镜像标签版本是latest，默认策略就是Always 如果指定特定版本默认拉取策略就是IfNotPresent。 将上面的pod删除再创建，使用下面命令查看信息 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 [root@k8s-master01 ~/dashboard]# kubectl delete -f pod1.yml pod \u0026#34;pod-stress\u0026#34; deleted [root@k8s-master01 ~/dashboard]# kubectl get pods No resources found in default namespace. [root@k8s-master01 ~/dashboard]# docker images | grep stress polinux/stress latest df58d15b053d 5 years ago 9.74MB [root@k8s-master01 ~/dashboard]# kubectl apply -f pod1.yml pod/pod-stress created [root@k8s-master01 ~/dashboard]# kubectl describe pod pod-stress ...... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 26s default-scheduler Successfully assigned default/pod-stress to k8s-worker02 Normal Pulling 25s kubelet Pulling image \u0026#34;polinux/stress\u0026#34; Normal Pulled 17s kubelet Successfully pulled image \u0026#34;polinux/stress\u0026#34; in 7.253s (7.253s including waiting) Normal Created 17s kubelet Created container c1 Normal Started 17s kubelet Started container c1 说明: 可以看到第二行信息还是pulling image下载镜像\n修改YAML 1 2 3 4 5 6 7 8 9 10 11 12 [root@k8s-master01 ~/dashboard]# cat pod1.yml apiVersion: v1 kind: Pod metadata: name: pod-stress spec: containers: - name: c1 image: polinux/stress command: [\u0026#34;stress\u0026#34;] args: [\u0026#34;--vm\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;--vm-bytes\u0026#34;, \u0026#34;150M\u0026#34;, \u0026#34;--vm-hang\u0026#34;, \u0026#34;1\u0026#34;] imagePullPolicy: IfNotPresent # 增加了这一句 再次删除再创建 1 2 3 4 5 6 7 8 9 10 11 12 13 [root@k8s-master01 ~/dashboard]# kubectl delete -f pod1.yml pod \u0026#34;pod-stress\u0026#34; deleted [root@k8s-master01 ~/dashboard]# kubectl apply -f pod1.yml pod/pod-stress created [root@k8s-master01 ~/dashboard]# kubectl describe pod pod-stress ...... Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 18s default-scheduler Successfully assigned default/pod-stress to k8s-worker02 Normal Pulled 17s kubelet Container image \u0026#34;polinux/stress\u0026#34; already present on machine Normal Created 17s kubelet Created container c1 Normal Started 17s kubelet Started container c1 说明: 第二行信息是说镜像已经存在，直接使用了\npod的标签 为pod设置label,用于控制器通过label与pod关联 语法与前面学的node标签几乎一致 通过命令管理Pod标签 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 [root@k8s-master01 ~]# kubectl get pod pod-stress --show-labels NAME READY STATUS RESTARTS AGE LABELS pod-stress 1/1 Running 0 4m58s \u0026lt;none\u0026gt; [root@k8s-master01 ~]# kubectl label pod pod-stress region=huanan zone=A env=test bussiness=game pod/pod-stress labeled [root@k8s-master01 ~]# kubectl get pod pod-stress --show-labels NAME READY STATUS RESTARTS AGE LABELS pod-stress 1/1 Running 0 7m27s bussiness=game,env=test,region=huanan,zone=A [root@k8s-master01 ~]# kubectl get pods -l zone=A NAME READY STATUS RESTARTS AGE pod-stress 1/1 Running 0 8m45s [root@k8s-master01 ~]# kubectl get pods -l \u0026#34;zone in (A,B,C)\u0026#34; NAME READY STATUS RESTARTS AGE pod-stress 1/1 Running 0 9m22s [root@k8s-master01 ~]# kubectl label pod pod-stress zone=B --overwrite=true pod/pod-stress labeled [root@k8s-master01 ~]# kubectl get pod pod-stress --show-labels NAME READY STATUS RESTARTS AGE LABELS pod-stress 1/1 Running 0 11m bussiness=game,env=test,region=huanan,zone=B [root@k8s-master01 ~]# kubectl label pod pod-stress bussiness- env- region- zone- pod/pod-stress unlabeled [root@k8s-master01 ~]# kubectl get pod pod-stress --show-labels NAME READY STATUS RESTARTS AGE LABELS pod-stress 1/1 Running 0 12m \u0026lt;none\u0026gt; 小结:\npod的label与node的label操作方式几乎相同 node的label用于pod调度到指定label的node节点 pod的label用于controller关联控制的pod 通过YAML创建Pod时添加标签 修改yaml\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: Pod metadata: name: pod-stress labels: # 增加多个标签 env: dev app: nginx spec: containers: - name: c1 image: polinux/stress command: [\u0026#34;stress\u0026#34;] args: [\u0026#34;--vm\u0026#34;, \u0026#34;1\u0026#34;, \u0026#34;--vm-bytes\u0026#34;, \u0026#34;150M\u0026#34;, \u0026#34;--vm-hang\u0026#34;, \u0026#34;1\u0026#34;] imagePullPolicy: IfNotPresent 直接apply应用\n1 2 [root@k8s-master01 ~/dashboard]# kubectl apply -f pod1.yml pod/pod-stress configured 验证\n1 2 3 [root@k8s-master01 ~/dashboard]# kubectl get pod pod-stress --show-labels NAME READY STATUS RESTARTS AGE LABELS pod-stress 1/1 Running 0 16m app=nginx,env=dev ","date":"2024-07-24T00:00:00Z","image":"https://caijemmy.github.io/p/3.0-kubernetes%E9%9B%86%E7%BE%A4%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5pod/cover_hu_e95a4276bf860a84.jpg","permalink":"https://caijemmy.github.io/p/3.0-kubernetes%E9%9B%86%E7%BE%A4%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5pod/","title":"3.0 Kubernetes集群核心概念Pod"},{"content":"kubernetes核心概念 Controller pod控制器controller Controller作用及分类 参考: 工作负载管理 | Kubernetes\nDeployment ","date":"2024-07-24T00:00:00Z","image":"https://caijemmy.github.io/p/4.0-kubernetes%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5controller/cover_hu_e95a4276bf860a84.jpg","permalink":"https://caijemmy.github.io/p/4.0-kubernetes%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5controller/","title":"4.0 kubernetes核心概念Controller"},{"content":"Kubernetes集群Node管理 查看集群信息 1 2 3 4 5 [root@k8s-master01 ~]# kubectl cluster-info Kubernetes control plane is running at https://192.168.52.129:6443 CoreDNS is running at https://192.168.52.129:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use \u0026#39;kubectl cluster-info dump\u0026#39;. 查看节点信息 查看集群节点信息 1 2 3 4 5 [root@k8s-master01 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master01 Ready control-plane 34h v1.28.15 k8s-worker01 Ready \u0026lt;none\u0026gt; 31h v1.28.15 k8s-worker02 Ready \u0026lt;none\u0026gt; 31h v1.28.15 查看集群节点详细信息 1 2 3 4 5 [root@k8s-master01 ~]# kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k8s-master01 Ready control-plane 34h v1.28.15 192.168.52.129 \u0026lt;none\u0026gt; Debian GNU/Linux 12 (bookworm) 6.1.0-34-amd64 docker://28.1.1 k8s-worker01 Ready \u0026lt;none\u0026gt; 31h v1.28.15 192.168.52.140 \u0026lt;none\u0026gt; Debian GNU/Linux 12 (bookworm) 6.1.0-34-amd64 docker://28.1.1 k8s-worker02 Ready \u0026lt;none\u0026gt; 31h v1.28.15 192.168.52.141 \u0026lt;none\u0026gt; Debian GNU/Linux 12 (bookworm) 6.1.0-34-amd64 docker://28.1.1 查看节点描述详细信息 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 [root@k8s-master01 ~]# kubectl describe node k8s-master01 Name: k8s-master01 Roles: control-plane Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/arch=amd64 kubernetes.io/hostname=k8s-master01 kubernetes.io/os=linux node-role.kubernetes.io/control-plane= node.kubernetes.io/exclude-from-external-load-balancers= Annotations: csi.volume.kubernetes.io/nodeid: {\u0026#34;csi.tigera.io\u0026#34;:\u0026#34;k8s-master01\u0026#34;} kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock node.alpha.kubernetes.io/ttl: 0 projectcalico.org/IPv4Address: 192.168.52.129/24 projectcalico.org/IPv4VXLANTunnelAddr: 10.244.32.128 volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Mon, 12 May 2025 00:14:17 -0400 Taints: node-role.kubernetes.io/control-plane:NoSchedule Unschedulable: false Lease: HolderIdentity: k8s-master01 AcquireTime: \u0026lt;unset\u0026gt; RenewTime: Tue, 13 May 2025 11:01:37 -0400 Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- NetworkUnavailable False Tue, 13 May 2025 10:56:59 -0400 Tue, 13 May 2025 10:56:59 -0400 CalicoIsUp Calico is running on this node MemoryPressure False Tue, 13 May 2025 10:56:41 -0400 Mon, 12 May 2025 00:14:16 -0400 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Tue, 13 May 2025 10:56:41 -0400 Mon, 12 May 2025 00:14:16 -0400 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Tue, 13 May 2025 10:56:41 -0400 Mon, 12 May 2025 00:14:16 -0400 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Tue, 13 May 2025 10:56:41 -0400 Mon, 12 May 2025 05:46:53 -0400 KubeletReady kubelet is posting ready status. AppArmor enabled Addresses: InternalIP: 192.168.52.129 Hostname: k8s-master01 Capacity: cpu: 4 ephemeral-storage: 101639152Ki hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 5284488Ki pods: 110 Allocatable: cpu: 4 ephemeral-storage: 93670642329 hugepages-1Gi: 0 hugepages-2Mi: 0 memory: 5182088Ki pods: 110 System Info: Machine ID: f0d5fce3fd2f48b7ae64f003370f9732 System UUID: 7bf54d56-e453-1f28-83fb-20ad43599d29 Boot ID: ec68e796-ffa6-4076-89f4-ebaae9433dbb Kernel Version: 6.1.0-34-amd64 OS Image: Debian GNU/Linux 12 (bookworm) Operating System: linux Architecture: amd64 Container Runtime Version: docker://28.1.1 Kubelet Version: v1.28.15 Kube-Proxy Version: v1.28.15 PodCIDR: 10.244.0.0/24 PodCIDRs: 10.244.0.0/24 Non-terminated Pods: (15 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age --------- ---- ------------ ---------- --------------- ------------- --- calico-apiserver calico-apiserver-5b9b48d497-5ck29 0 (0%) 0 (0%) 0 (0%) 0 (0%) 34h calico-apiserver calico-apiserver-5b9b48d497-lm8jm 0 (0%) 0 (0%) 0 (0%) 0 (0%) 34h calico-system calico-kube-controllers-789b9578b6-296xf 0 (0%) 0 (0%) 0 (0%) 0 (0%) 34h calico-system calico-node-vll2x 250m (6%) 0 (0%) 0 (0%) 0 (0%) 27h calico-system calico-typha-5dd4d5c669-7mcsb 0 (0%) 0 (0%) 0 (0%) 0 (0%) 27h calico-system csi-node-driver-gwsql 0 (0%) 0 (0%) 0 (0%) 0 (0%) 34h calico-system goldmane-8456f8bf4d-vdc4h 0 (0%) 0 (0%) 0 (0%) 0 (0%) 34h kube-system coredns-5dd5756b68-gbgsh 100m (2%) 0 (0%) 70Mi (1%) 170Mi (3%) 34h kube-system coredns-5dd5756b68-pm85d 100m (2%) 0 (0%) 70Mi (1%) 170Mi (3%) 34h kube-system etcd-k8s-master01 100m (2%) 0 (0%) 100Mi (1%) 0 (0%) 34h kube-system kube-apiserver-k8s-master01 250m (6%) 0 (0%) 0 (0%) 0 (0%) 34h kube-system kube-controller-manager-k8s-master01 200m (5%) 0 (0%) 0 (0%) 0 (0%) 34h kube-system kube-proxy-qt8px 0 (0%) 0 (0%) 0 (0%) 0 (0%) 26h kube-system kube-scheduler-k8s-master01 100m (2%) 0 (0%) 0 (0%) 0 (0%) 34h tigera-operator tigera-operator-5b59476cb-gnd5z 0 (0%) 0 (0%) 0 (0%) 0 (0%) 34h Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 1100m (27%) 0 (0%) memory 240Mi (4%) 340Mi (6%) ephemeral-storage 0 (0%) 0 (0%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Starting 115m kube-proxy Normal Starting 4m56s kube-proxy Normal NodeHasSufficientMemory 117m (x8 over 117m) kubelet Node k8s-master01 status is now: NodeHasSufficientMemory Warning InvalidDiskCapacity 117m kubelet invalid capacity 0 on image filesystem Normal NodeHasNoDiskPressure 117m (x7 over 117m) kubelet Node k8s-master01 status is now: NodeHasNoDiskPressure Normal NodeHasSufficientPID 117m (x7 over 117m) kubelet Node k8s-master01 status is now: NodeHasSufficientPID Normal NodeAllocatableEnforced 117m kubelet Updated Node Allocatable limit across pods Normal Starting 117m kubelet Starting kubelet. Normal RegisteredNode 116m node-controller Node k8s-master01 event: Registered Node k8s-master01 in Controller Normal Starting 5m7s kubelet Starting kubelet. Warning InvalidDiskCapacity 5m7s kubelet invalid capacity 0 on image filesystem Normal NodeHasSufficientMemory 5m7s (x8 over 5m7s) kubelet Node k8s-master01 status is now: NodeHasSufficientMemory Normal NodeHasNoDiskPressure 5m7s (x7 over 5m7s) kubelet Node k8s-master01 status is now: NodeHasNoDiskPressure Normal NodeHasSufficientPID 5m7s (x7 over 5m7s) kubelet Node k8s-master01 status is now: NodeHasSufficientPID Normal NodeAllocatableEnforced 5m7s kubelet Updated Node Allocatable limit across pods Normal RegisteredNode 4m32s node-controller Node k8s-master01 event: Registered Node k8s-master01 in Controller worker node节点管理集群 如果是kubeasz安装，所有节点(包括master与node)都已经可以对集群进行管理\n如果是kubeadm安装，在node节点上管理时会报如下错误\n1 2 [root@k8s-worker01 ~]# kubectl get nodes The connection to the server localhost:8080 was refused - did you specify the right host or port? 只要把master上的管理文件/etc/kubernetes/admin.conf拷贝到node节点的$HOME/.kube/config就可以让node节点也可以实现kubectl命令管理\n1, 在node节点的用户家目录创建.kube目录\n1 [root@k8s-worker01 ~]# mkdir /root/.kube 2, 在worker节点做如下操作\n1 [root@k8s-worker01 ~]# [ ! -f /root/.kube/config ] \u0026amp;\u0026amp; scp k8s-master01:/etc/kubernetes/admin.conf /root/.kube/config 3, 在worker node节点验证\n1 2 3 4 5 [root@k8s-worker01 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master01 Ready control-plane 34h v1.28.15 k8s-worker01 Ready \u0026lt;none\u0026gt; 31h v1.28.15 k8s-worker02 Ready \u0026lt;none\u0026gt; 31h v1.28.15 节点标签(label) k8s集群如果由大量节点组成，可将节点打上对应的标签，然后通过标签进行筛选及查看,更好的进行资源对象的相关选择与匹配 查看节点标签信息 1 2 3 4 5 [root@k8s-master01 ~]# kubectl get node --show-labels NAME STATUS ROLES AGE VERSION LABELS k8s-master01 Ready control-plane 34h v1.28.15 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master01,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers= k8s-worker01 Ready \u0026lt;none\u0026gt; 31h v1.28.15 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-worker01,kubernetes.io/os=linux k8s-worker02 Ready \u0026lt;none\u0026gt; 31h v1.28.15 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-worker02,kubernetes.io/os=linux 设置节点标签信息 设置节点标签 为节点k8s-worker01打一个region=cainiao 的标签\n1 2 [root@k8s-master01 ~]# kubectl label node k8s-worker01 region=cainiao node/k8s-worker01 labeled 查看所有节点标签 1 2 3 4 5 [root@k8s-master01 ~]# kubectl get node --show-labels NAME STATUS ROLES AGE VERSION LABELS k8s-master01 Ready control-plane 34h v1.28.15 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-master01,kubernetes.io/os=linux,node-role.kubernetes.io/control-plane=,node.kubernetes.io/exclude-from-external-load-balancers= k8s-worker01 Ready \u0026lt;none\u0026gt; 31h v1.28.15 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-worker01,kubernetes.io/os=linux,region=cainiao k8s-worker02 Ready \u0026lt;none\u0026gt; 31h v1.28.15 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-worker02,kubernetes.io/os=linux 查看所有节点带region的标签 1 2 3 4 5 [root@k8s-master01 ~]# kubectl get nodes -L region NAME STATUS ROLES AGE VERSION REGION k8s-master01 Ready control-plane 34h v1.28.15 k8s-worker01 Ready \u0026lt;none\u0026gt; 31h v1.28.15 cainiao k8s-worker02 Ready \u0026lt;none\u0026gt; 31h v1.28.15 多维度标签 设置多维度标签 也可以加其它的多维度标签,用于不同的需要区分的场景\n如把k8s-worker02标签为华南区,A机房,测试环境,游戏业务\n1 2 [root@k8s-master01 ~]# kubectl label node k8s-worker02 zone=A env=test bussiness=game node/k8s-worker02 labeled 1 2 3 [root@k8s-master01 ~]# kubectl get nodes k8s-worker02 --show-labels NAME STATUS ROLES AGE VERSION LABELS k8s-worker02 Ready \u0026lt;none\u0026gt; 31h v1.28.15 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,bussiness=game,env=test,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-worker02,kubernetes.io/os=linux,zone=A 显示节点的相应标签 1 2 3 4 5 [root@k8s-master01 ~]# kubectl get nodes -L region,zone NAME STATUS ROLES AGE VERSION REGION ZONE k8s-master01 Ready control-plane 35h v1.28.15 k8s-worker01 Ready \u0026lt;none\u0026gt; 31h v1.28.15 cainiao k8s-worker02 Ready \u0026lt;none\u0026gt; 31h v1.28.15 A 查找region=cainiao的节点 1 2 3 [root@k8s-master01 ~]# kubectl get nodes -l region=cainiao NAME STATUS ROLES AGE VERSION k8s-worker01 Ready \u0026lt;none\u0026gt; 31h v1.28.15 标签的修改 1 2 3 4 [root@k8s-master01 ~]# kubectl label node k8s-worker02 bussiness=ad --overwrite=true node/k8s-worker02 labeled 加上--overwrite=true覆盖原标签的value进行修改操作 1 2 3 [root@k8s-master01 ~]# kubectl get node k8s-worker02 -L bussiness NAME STATUS ROLES AGE VERSION BUSSINESS k8s-worker02 Ready \u0026lt;none\u0026gt; 31h v1.28.15 ad 标签的删除 使用key加一个减号的写法来取消标签\n1 2 3 4 5 6 [root@k8s-master01 ~]# kubectl label node k8s-worker01 region- node/k8s-worker01 unlabeled [root@k8s-master01 ~]# kubectl get nodes k8s-worker01 --show-labels NAME STATUS ROLES AGE VERSION LABELS k8s-worker01 Ready \u0026lt;none\u0026gt; 32h v1.28.15 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=k8s-worker01,kubernetes.io/os=linux 标签选择器 标签选择器主要有2类:\n等值关系: =, != 集合关系: KEY in {VALUE1, VALUE2\u0026hellip;\u0026hellip;} 1 2 3 4 5 [root@k8s-master01 ~]# kubectl label node k8s-master01 env=test1 node/k8s-master01 labeled [root@k8s-master01 ~]# kubectl label node k8s-worker01 env=test2 node/k8s-worker01 labeled 1 2 3 4 [root@k8s-master01 ~]# kubectl get node -l \u0026#34;env in (test1,test2)\u0026#34; NAME STATUS ROLES AGE VERSION k8s-master01 Ready control-plane 35h v1.28.15 k8s-worker01 Ready \u0026lt;none\u0026gt; 32h v1.28.15 ","date":"2024-07-23T00:00:00Z","image":"https://caijemmy.github.io/p/2.0-kubernetes%E9%9B%86%E7%BE%A4node%E7%AE%A1%E7%90%86/cover_hu_e95a4276bf860a84.jpg","permalink":"https://caijemmy.github.io/p/2.0-kubernetes%E9%9B%86%E7%BE%A4node%E7%AE%A1%E7%90%86/","title":"2.0 Kubernetes集群Node管理"},{"content":"kubeadm部署单Master节点kubernetes集群 kubernetes 1.28.15 部署环境准备 主机操作系统说明 序号 操作系统及版本 备注 1 Debian GNU/Linux 12 (bookworm) Debian 6.1.135-1 (2025-04-25) 主机硬件配置说明 需求 CPU 内存 硬盘 角色 主机名 值 4C 4G 100GB master k8s-master01 值 4C 4G 100GB worker(node) k8s-worker01 值 4C 4G 100GB worker(node) k8s-worker02 主机配置 配置root 账号可以直接登录 1 2 3 4 5 # 虚机配置过程，已经提炼为shell脚本，参考 VM/init_vm.sh # vim /etc/ssh/sshd_config PermitRootLogin yes systemctl restart ssh 主机名配置 1 2 3 4 5 6 7 8 # master节点,名称为k8s-master01 hostnamectl set-hostname k8s-master01 # worker1节点,名称为k8s-worker01 hostnamectl set-hostname k8s-worker01 # worker2节点,名称为k8s-worker02 hostnamectl set-hostname k8s-worker02 主机IP地址配置 NetworkManager 的配置文件位于 /etc/NetworkManager/system-connections/ 目录下。可以通过编辑配置文件来设置静态 IP。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 # 方式1：修改以下部分： cd /etc/NetworkManager/system-connections/ mv \u0026#34;Wired connection 1\u0026#34; master01_ens33.nmconnection [ipv4] method=manual address1=192.168.52.129/24,192.168.52.2 dns=192.168.52.2;8.8.8.8; # 运行以下命令重新加载配置： sudo nmcli connection reload sudo nmcli connection down master01_ens33 sudo nmcli connection up master01_ens33 # 方式2：命令修改以下部分： cd /etc/NetworkManager/system-connections/ mv \u0026#34;Wired connection 1\u0026#34; master01_ens33.nmconnection sudo nmcli connection reload sudo nmcli connection modify master01_ens33 \\ ipv4.method manual \\ ipv4.addresses 192.168.52.129/24 \\ ipv4.gateway 192.168.52.2 \\ ipv4.dns \u0026#34;192.168.52.2 8.8.8.8\u0026#34; # 激活连接 sudo nmcli connection down master01_ens33 sudo nmcli connection up master01_ens33 sudo nmcli connection down master01_ens33 \u0026amp;\u0026amp; sudo nmcli connection up master01_ens33 主机名与IP地址解析 1 2 3 4 5 6 7 8 # 所有集群主机均需要进行配置。 # cat /etc/hosts 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.52.129 k8s-master01 192.168.52.140 k8s-worker01 192.168.52.141 k8s-worker02 # 验证： ping -c 4 k8s-master01 \u0026amp;\u0026amp; ping -c 4 k8s-worker01 \u0026amp;\u0026amp; ping -c 4 k8s-worker02 防火墙配置 1 2 3 4 5 6 7 8 9 10 11 12 # 所有主机均需要操作。 # 在 Debian 12 中，默认的防火墙工具是 nftables（替代了之前的 iptables）。以下是关闭防火墙和查看防火墙状态的命令： # 检查 nftables 状态 sudo nft list ruleset # 检查 firewalld 状态（如果安装了） sudo firewall-cmd --state # 检查 ufw 状态（如果安装了） sudo ufw status systemctl status nftables.service SELINUX配置 1 2 3 4 5 6 7 # 所有主机均需要操作。修改SELinux配置需要重启操作系统。 # sed -ri \u0026#39;s/SELINUX=enforcing/SELINUX=disabled/\u0026#39; /etc/selinux/config # 在 Debian 系统中，默认不安装 SELinux。 sudo aa-status # 如果输出显示 apparmor module is loaded，表示 AppArmor 正在运行。 # 在 Debian 系统上安装 Kubernetes 时，通常不需要关闭 AppArmor。AppArmor 与 Kubernetes 和常见容器运行时兼容， # 并提供额外的安全性。只有在特定情况下（如配置不当或兼容性问题）才需要关闭 AppArmor。 时间同步配置 1 2 3 4 # 所有主机均需要操作。最小化安装系统需要安装ntpdate软件。 # apt -y install ntpdate crontab -l 0 */1 * * * /usr/sbin/ntpdate time1.aliyun.com 升级操作系统内核 1 2 3 # 所有主机均需要操作。 sudo apt update sudo apt upgrade 配置内核转发及网桥过滤 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 所有主机均需要操作。 # 添加网桥过滤及内核转发配置文件 # cat /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 vm.swappiness = 0 # 加载br_netfilter模块 modprobe br_netfilter # 查看是否加载 # lsmod | grep br_netfilter br_netfilter 22256 0 bridge 151336 1 br_netfilter # 加载网桥过滤及内核转发配置文件 # sysctl -p /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 vm.swappiness = 0 安装ipset及ipvsadm 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # 所有主机均需要操作。主要用于实现service转发。 # 安装ipset及ipvsadm apt -y install ipset ipvsadm # 配置ipvsadm模块加载方式 # 添加需要加载的模块，这里需要把br_netfilter也加上，减少一个配置内核转发及网桥过滤的单独服务。 cat \u0026gt; /etc/ipvsadm.rules \u0026lt;\u0026lt;EOF #!/bin/bash modprobe -- br_netfilter modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack EOF # 授权、运行、检查是否加载 chmod 755 /etc/ipvsadm.rules \u0026amp;\u0026amp; bash /etc/ipvsadm.rules \u0026amp;\u0026amp; lsmod | grep -e ip_vs -e nf_conntrack vim /etc/systemd/system/ipvsadm.service # 将启动内容拷贝 [Unit] Description=IPVS Load Balancer After=network.target [Service] Type=oneshot ExecStart=/bin/bash /etc/ipvsadm.rules ExecStop=/sbin/ipvsadm -C RemainAfterExit=yes [Install] WantedBy=multi-user.target sudo systemctl enable ipvsadm sudo systemctl start ipvsadm sudo systemctl status ipvsadm 关闭SWAP分区 1 2 3 4 5 6 7 8 9 # 修改完成后需要重启操作系统，如不重启，可临时关闭，命令为swapoff -a # 永远关闭swap分区，需要重启操作系统 # cat /etc/fstab ...... # /dev/mapper/centos-swap swap swap defaults 0 0 # 在上一行中行首添加# Docker准备 所有集群主机均需操作。 Debian安装docker官网参考\n卸载已安装decker 1 2 3 4 sudo apt-get purge docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-ce-rootless-extras sudo rm -rf /var/lib/docker sudo rm -rf /var/lib/containerd 安装docker 阿里云docker源及安装配置参考\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # step 1: 安装必要的一些系统工具 sudo apt-get update sudo apt-get -y install ca-certificates curl gnupg # step 2: 信任 Docker 的 GPG 公钥 sudo install -m 0755 -d /etc/apt/keyrings curl -fsSL https://mirrors.aliyun.com/docker-ce/linux/debian/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg sudo chmod a+r /etc/apt/keyrings/docker.gpg # Step 3: 写入软件源信息 echo \\ \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://mirrors.aliyun.com/docker-ce/linux/debian \\ \u0026#34;$(. /etc/os-release \u0026amp;\u0026amp; echo \u0026#34;$VERSION_CODENAME\u0026#34;)\u0026#34; stable\u0026#34; | \\ sudo tee /etc/apt/sources.list.d/docker.list \u0026gt; /dev/null # Step 4: 安装Docker sudo apt-get update sudo apt-get -y install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin 修改cgroup方式 1 2 3 4 5 6 7 8 9 # 在/etc/docker/daemon.json添加如下内容 # cat /etc/docker/daemon.json { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;] } # 重启docker systemctl restart docker 安装cri-dockerd Releases · Mirantis/cri-dockerd\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # 每个节点都安装 wget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.17/cri-dockerd_0.3.17.3-0.debian-bookworm_amd64.deb dpkg -i cri-dockerd_0.3.17.3-0.debian-bookworm_amd64.deb dpkg -l | grep cri-docker # vim /usr/lib/systemd/system/cri-docker.service # 修改第10行内容 # --pod-infra-container-image：指定 Kubernetes 使用的 pause 容器镜像，确保 Pod 的基础功能正常运行。 # --container-runtime-endpoint：指定 cri-docker 与容器运行时的通信方式，确保 Kubernetes 能够管理容器。 ExecStart=/usr/bin/cri-dockerd --pod-infra-container-image=registry.k8s.io/pause:3.9 --container-runtime-endpoint fd:// sudo systemctl start cri-docker sudo systemctl enable cri-docker sudo systemctl status cri-docker kubernetes 1.28.X 集群部署 集群软件及版本说明 kubeadm kubelet kubectl 版本 1.28.X 1.28.X 1.28.X 安装位置 集群所有主机 集群所有主机 集群所有主机 作用 初始化集群、管理集群等 用于接收api-server指令，对pod生命周期进行管理 集群应用命令行管理工具 阿里源安装集群软件 1 2 3 4 5 6 7 8 # debian 系统安装 apt-get update \u0026amp;\u0026amp; apt-get install -y apt-transport-https curl -fsSL https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.28/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg echo \u0026#34;deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://mirrors.aliyun.com/kubernetes-new/core/stable/v1.28/deb/ /\u0026#34; | tee /etc/apt/sources.list.d/kubernetes.list apt-get update apt-get install -y kubelet kubeadm kubectl 配置kubelet 为了实现docker使用的cgroupdriver与kubelet使用的cgroup的一致性，建议修改如下文件内容。\n1 2 3 4 5 6 # vim /etc/sysconfig/kubelet KUBELET_EXTRA_ARGS=\u0026#34;--cgroup-driver=systemd\u0026#34; # debian 系统 /etc/default/kubelet systemctl enable kubelet 集群镜像准备 必须科学上网，否则都是徒劳\n1 2 3 4 5 # 查看要下载的镜像信息 kubeadm config images list # 使用下面的命令进行镜像下载 kubeadm config images pull --cri-socket unix:///var/run/cri-dockerd.sock 集群初始化 1 2 3 4 5 kubeadm init --kubernetes-version=v1.28.15 --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=192.168.52.129 --cri-socket unix:///var/run/cri-dockerd.sock # 如果不添加--cri-socket选项，则会报错，内容如下： # Found multiple CRI endpoints on the host. Please define which one do you wish to use by setting the \u0026#39;criSocket\u0026#39; field in the kubeadm configuration file: unix:///var/run/containerd/containerd.sock, unix:///var/run/cri-dockerd.sock # To see the stack trace of this error execute with --v=5 or higher 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 # 配置输出 [init] Using Kubernetes version: v1.28.15 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using \u0026#39;kubeadm config images pull\u0026#39; W0504 12:48:00.026524 2144 checks.go:835] detected that the sandbox image \u0026#34;registry.k8s.io/pause:3.8\u0026#34; of the container runtime is inconsistent with that used by kubeadm. It is recommended that using \u0026#34;registry.k8s.io/pause:3.9\u0026#34; as the CRI sandbox image. [certs] Using certificateDir folder \u0026#34;/etc/kubernetes/pki\u0026#34; [certs] Generating \u0026#34;ca\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver\u0026#34; certificate and key [certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local master01] and IPs [10.96.0.1 192.168.52.129] [certs] Generating \u0026#34;apiserver-kubelet-client\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-ca\u0026#34; certificate and key [certs] Generating \u0026#34;front-proxy-client\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/ca\u0026#34; certificate and key [certs] Generating \u0026#34;etcd/server\u0026#34; certificate and key [certs] etcd/server serving cert is signed for DNS names [localhost master01] and IPs [192.168.52.129 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/peer\u0026#34; certificate and key [certs] etcd/peer serving cert is signed for DNS names [localhost master01] and IPs [192.168.52.129 127.0.0.1 ::1] [certs] Generating \u0026#34;etcd/healthcheck-client\u0026#34; certificate and key [certs] Generating \u0026#34;apiserver-etcd-client\u0026#34; certificate and key [certs] Generating \u0026#34;sa\u0026#34; key and public key [kubeconfig] Using kubeconfig folder \u0026#34;/etc/kubernetes\u0026#34; [kubeconfig] Writing \u0026#34;admin.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;kubelet.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;controller-manager.conf\u0026#34; kubeconfig file [kubeconfig] Writing \u0026#34;scheduler.conf\u0026#34; kubeconfig file [etcd] Creating static Pod manifest for local etcd in \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Using manifest folder \u0026#34;/etc/kubernetes/manifests\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-apiserver\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-controller-manager\u0026#34; [control-plane] Creating static Pod manifest for \u0026#34;kube-scheduler\u0026#34; [kubelet-start] Writing kubelet environment file with flags to file \u0026#34;/var/lib/kubelet/kubeadm-flags.env\u0026#34; [kubelet-start] Writing kubelet configuration to file \u0026#34;/var/lib/kubelet/config.yaml\u0026#34; [kubelet-start] Starting the kubelet [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \u0026#34;/etc/kubernetes/manifests\u0026#34;. This can take up to 4m0s [apiclient] All control plane components are healthy after 21.505666 seconds [upload-config] Storing the configuration used in ConfigMap \u0026#34;kubeadm-config\u0026#34; in the \u0026#34;kube-system\u0026#34; Namespace [kubelet] Creating a ConfigMap \u0026#34;kubelet-config\u0026#34; in namespace kube-system with the configuration for the kubelets in the cluster [upload-certs] Skipping phase. Please see --upload-certs [mark-control-plane] Marking the node master01 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers] [mark-control-plane] Marking the node master01 as control-plane by adding the taints [node-role.kubernetes.io/control-plane:NoSchedule] [bootstrap-token] Using token: o4l5gb.8jem7v7x9upd376t [bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes [bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials [bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token [bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster [bootstrap-token] Creating the \u0026#34;cluster-info\u0026#34; ConfigMap in the \u0026#34;kube-public\u0026#34; namespace [kubelet-finalize] Updating \u0026#34;/etc/kubernetes/kubelet.conf\u0026#34; to point to a rotatable kubelet client certificate and key [addons] Applied essential addon: CoreDNS [addons] Applied essential addon: kube-proxy Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.52.129:6443 --token ghwg0j.hwc8l4o3h8j65le4 \\ --discovery-token-ca-cert-hash sha256:31ae1608dded0e8a442eaa653e52fc371f46bcbe3b047aa4a7a0a218d2601a50 集群应用客户端管理集群文件准备 1 2 3 4 5 # 根据提示执行命令 [root@k8s-master01 ~]# mkdir -p $HOME/.kube [root@k8s-master01 ~]# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config [root@k8s-master01 ~]# chown $(id -u):$(id -g) $HOME/.kube/config [root@k8s-master01 ~]# export KUBECONFIG=/etc/kubernetes/admin.conf 集群网络插件部署 calico 使用calico部署集群网络\n安装参考网址：Tutorial: Install Calico on single-host k8s cluster | Calico Documentation\n1 2 # 应用operator资源清单文件 [root@k8s-master01 ~]# kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.30.0/manifests/tigera-operator.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 下载自定义资源文件 [root@k8s-master01 ~]# wget https://raw.githubusercontent.com/projectcalico/calico/v3.30.0/manifests/custom-resources.yaml # 修改配置 # 修改文件第13行，修改为使用kubeadm init ----pod-network-cidr对应的IP地址段 [root@k8s-master01 ~]# vim custom-resources.yaml ...... 11 ipPools: 12 - blockSize: 26 13 cidr: 10.244.0.0/16 14 encapsulation: VXLANCrossSubnet ...... # 应用资源清单文件 [root@k8s-master01 ~]# kubectl create -f custom-resources.yaml 1 2 # 监视calico-sysem命名空间中pod运行情况 [root@k8s-master01 ~]# watch kubectl get pods -n calico-system Wait until each pod has the STATUS of Running.\n1、科学上网很重要\n2、重启虚机，重启主机，科学上网断开重连\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 已经全部运行 [root@k8s-master01 ~]# kubectl get pods -n calico-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-789b9578b6-296xf 1/1 Running 11 (21h ago) 2d21h calico-node-hrqlr 1/1 Running 5 (21h ago) 2d14h calico-node-l7dhd 1/1 Running 5 (21h ago) 2d14h calico-node-vll2x 1/1 Running 5 (21h ago) 2d14h calico-typha-5dd4d5c669-49svd 1/1 Running 5 (21h ago) 2d14h calico-typha-5dd4d5c669-7mcsb 1/1 Running 5 (21h ago) 2d14h csi-node-driver-fcx7c 2/2 Running 10 (21h ago) 2d19h csi-node-driver-gwsql 2/2 Running 10 (21h ago) 2d21h csi-node-driver-qlx46 2/2 Running 10 (21h ago) 2d19h goldmane-8456f8bf4d-vdc4h 1/1 Running 5 (21h ago) 2d21h whisker-594579fbdd-dtbjt 2/2 Running 10 (21h ago) 2d14h 集群工作节点添加 因容器镜像下载较慢，可能会导致报错，主要错误为没有准备好cni（集群网络插件），如有网络，请耐心等待即可。\n1 2 [root@k8s-worker01 ~]# kubeadm join 192.168.52.129:6443 --token ghwg0j.hwc8l4o3h8j65le4 \\ --discovery-token-ca-cert-hash sha256:31ae1608dded0e8a442eaa653e52fc371f46bcbe3b047aa4a7a0a218d2601a50 1 2 [root@k8s-worker02 ~]# kubeadm join 192.168.52.129:6443 --token ghwg0j.hwc8l4o3h8j65le4 \\ --discovery-token-ca-cert-hash sha256:31ae1608dded0e8a442eaa653e52fc371f46bcbe3b047aa4a7a0a218d2601a50 验证集群可用性 1 2 3 4 5 6 # 查看所有的节点 [root@k8s-master01 ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master01 Ready control-plane 2d22h v1.28.15 k8s-worker01 Ready \u0026lt;none\u0026gt; 2d19h v1.28.15 k8s-worker02 Ready \u0026lt;none\u0026gt; 2d19h v1.28.15 1 2 3 4 5 6 7 # 查看集群健康情况 [root@k8s-master01 ~]# kubectl get cs Warning: v1 ComponentStatus is deprecated in v1.19+ NAME STATUS MESSAGE ERROR controller-manager Healthy ok scheduler Healthy ok etcd-0 Healthy ok 1 2 3 4 5 6 7 8 9 10 11 12 # 查看kubernetes集群pod运行情况 [root@k8s-master01 ~]# kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-5dd5756b68-gbgsh 1/1 Running 11 (21h ago) 2d22h coredns-5dd5756b68-pm85d 1/1 Running 11 (21h ago) 2d22h etcd-k8s-master01 1/1 Running 11 (21h ago) 2d22h kube-apiserver-k8s-master01 1/1 Running 12 (21h ago) 2d22h kube-controller-manager-k8s-master01 1/1 Running 11 (21h ago) 2d22h kube-proxy-h7s9b 1/1 Running 5 (21h ago) 2d14h kube-proxy-qt8px 1/1 Running 5 (21h ago) 2d14h kube-proxy-wlvrg 1/1 Running 5 (21h ago) 2d14h kube-scheduler-k8s-master01 1/1 Running 11 (21h ago) 2d22h 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 再次查看calico-system命名空间中pod运行情况。 [root@k8s-master01 ~]# kubectl get pods -n calico-system NAME READY STATUS RESTARTS AGE calico-kube-controllers-789b9578b6-296xf 1/1 Running 11 (21h ago) 2d22h calico-node-hrqlr 1/1 Running 5 (21h ago) 2d14h calico-node-l7dhd 1/1 Running 5 (21h ago) 2d14h calico-node-vll2x 1/1 Running 5 (21h ago) 2d14h calico-typha-5dd4d5c669-49svd 1/1 Running 5 (21h ago) 2d14h calico-typha-5dd4d5c669-7mcsb 1/1 Running 5 (21h ago) 2d14h csi-node-driver-fcx7c 2/2 Running 10 (21h ago) 2d19h csi-node-driver-gwsql 2/2 Running 10 (21h ago) 2d22h csi-node-driver-qlx46 2/2 Running 10 (21h ago) 2d19h goldmane-8456f8bf4d-vdc4h 1/1 Running 5 (21h ago) 2d22h whisker-594579fbdd-dtbjt 2/2 Running 10 (21h ago) 2d14h 环境准备过程中问题排查 IP地址配置的问题 /etc/network/interfaces 这个下面不要做配置，不生效.\n使用界面做的配置 Debian 12 默认使用 NetworkManager 或 systemd-networkd 管理网络，而不是传统的 ifupdown 工具。 如果 NetworkManager 或 systemd-networkd 正在管理网络接口，/etc/network/interfaces 的配置会被忽略。\n以下是排查过程 NetworkManager 正在管理该接口\n1 2 3 4 5 6 7 8 9 10 11 12 13 root@master01:~# nmcli device status DEVICE TYPE STATE CONNECTION ens33 ethernet connected Wired connection 1 lo loopback connected (externally) lo # 查看 systemd-networkd 是否启用 root@master01:~# systemctl status systemd-networkd ○ systemd-networkd.service - Network Configuration Loaded: loaded (/lib/systemd/system/systemd-networkd.service; disabled; preset: enabled) Active: inactive (dead) TriggeredBy: ○ systemd-networkd.socket Docs: man:systemd-networkd.service(8) man:org.freedesktop.network1(5) 虚机重启后，ipvs和br_netfilter未被加载 虚机重启后，需要重新加载br_netfilter模块。\n将 modprobe \u0026ndash; br_netfilter 命令加入到 /etc/ipvsadm.rules\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 vim /etc/systemd/system/ipvsadm.service # 将启动内容拷贝 [Unit] Description=IPVS Load Balancer After=network.target [Service] Type=oneshot # 启动时不能执行这条命令：/sbin/ipvsadm-restore \u0026lt; /etc/ipvsadm.rules # 应该使用bash执行相应的规则文件 ExecStart=/bin/bash /etc/ipvsadm.rules ExecStop=/sbin/ipvsadm -C RemainAfterExit=yes [Install] WantedBy=multi-user.target [ERROR CRI]: container runtime is not running 1 2 3 4 5 6 7 8 9 # 初始化时报这个错 [init] Using Kubernetes version: v1.28.15 [preflight] Running pre-flight checks error execution phase preflight: [preflight] Some fatal errors occurred: [ERROR CRI]: container runtime is not running: output: time=\u0026#34;2025-05-04T11:52:11-04:00\u0026#34; level=fatal msg=\u0026#34;validate service connection: validate CRI v1 runtime API for endpoint \\\u0026#34;unix:///var/run/containerd/containerd.sock\\\u0026#34;: rpc error: code = Unimplemented desc = unknown service runtime.v1.RuntimeService\u0026#34; , error: exit status 1 [ERROR FileContent--proc-sys-net-bridge-bridge-nf-call-iptables]: /proc/sys/net/bridge/bridge-nf-call-iptables does not exist [preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...` To see the stack trace of this error execute with --v=5 or higher 排查过程及解决方法，内核转发及网桥过滤模块没有加载，统一在主机配置 \u0026gt; 安装ipset及ipvsadm步骤进行优化了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 # 解决容器运行时问题 # (1) 检查 containerd.md 是否已安装并运行 # 运行以下命令检查 containerd.md 的状态： sudo systemctl status containerd.md # (2) 验证 containerd.md 配置 # 确保 containerd.md 的配置文件（通常位于 /etc/containerd.md/config.toml）正确配置了 CRI（容器运行时接口）。运行以下命令生成默认配置（如果配置文件不存在）： sudo containerd.md config default | sudo tee /etc/containerd.md/config.toml # (3) 重启 containerd.md # 修改配置后，重启 containerd.md： sudo systemctl restart containerd.md # (4) 验证 containerd.md 是否支持 CRI # 运行以下命令验证 containerd.md 是否支持 CRI： sudo ctr version # 如果输出显示版本信息，说明 containerd.md 已正确配置。 # 解决 /proc/sys/net/bridge/bridge-nf-call-iptables 问题 # (1) 加载 br_netfilter 内核模块 # 运行以下命令加载 br_netfilter 模块： sudo modprobe br_netfilter # (2) 确保模块已加载 # 运行以下命令检查模块是否已加载： lsmod | grep br_netfilter # 如果输出显示 br_netfilter，说明模块已加载。 # (3) 确保 /proc/sys/net/bridge/bridge-nf-call-iptables 存在 # 运行以下命令检查文件是否存在： cat /proc/sys/net/bridge/bridge-nf-call-iptables # 如果文件不存在，可能是内核未正确配置。可以手动创建并设置值： echo 1 | sudo tee /proc/sys/net/bridge/bridge-nf-call-iptables # (4) 永久生效 # 为了确保每次重启后配置仍然有效，编辑 /etc/sysctl.conf 文件： sudo vim /etc/sysctl.conf # 添加以下内容： net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 # 保存并退出，然后应用配置： sudo sysctl --system kubectl get pods 状态未Running 1 2 [root@k8s-master01 ~]# kubectl describe pod calico-apiserver-5b9b48d497-5ck29 -n calico-apiserver network is not ready: container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized 1 2 3 4 5 6 7 8 9 # 这个目录下没有配置文件，说明没有安装 CNI ls /etc/cni/net.d/ wget https://docs.projectcalico.org/manifests/calico.yaml # 修改配置文件中的这个配置项 - name: CALICO_IPV4POOL_CIDR value: \u0026#34;10.244.0.0/16\u0026#34; kubectl apply -f calico.yaml ","date":"2024-07-22T00:00:00Z","image":"https://caijemmy.github.io/p/1.0-kubeadm%E9%83%A8%E7%BD%B2%E5%8D%95master%E8%8A%82%E7%82%B9kubernetes%E9%9B%86%E7%BE%A4/cover_hu_e95a4276bf860a84.jpg","permalink":"https://caijemmy.github.io/p/1.0-kubeadm%E9%83%A8%E7%BD%B2%E5%8D%95master%E8%8A%82%E7%82%B9kubernetes%E9%9B%86%E7%BE%A4/","title":"1.0 kubeadm部署单Master节点kubernetes集群"},{"content":"Kubernetes集群UI及主机资源监控 Kubernetes dashboard作用 通过dashboard能够直观了解Kubernetes集群中运行的资源对象 通过dashboard可以直接管理（创建、删除、重启等操作）资源对象 安装kubernetes dashboard 参考链接：master的 dashboard/README.md ·kubernetes/dashboard\nhelm参考链接：Helm | Installing Helm\n通过helm工具安装kubernetes dashboard 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 安装helm curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg \u0026gt; /dev/null sudo apt-get install apt-transport-https --yes echo \u0026#34;deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main\u0026#34; | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list sudo apt-get update sudo apt-get install helm helm version # 依赖科学上网 # Add kubernetes-dashboard repository [root@k8s-master01 ~/dashboard]# helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/ \u0026#34;kubernetes-dashboard\u0026#34; has been added to your repositories # Deploy a Helm Release named \u0026#34;kubernetes-dashboard\u0026#34; using the kubernetes-dashboard chart [root@k8s-master01 ~/dashboard]# helm upgrade --install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard --create-namespace --namespace kubernetes-dashboard 命令执行后的回显 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 Release \u0026#34;kubernetes-dashboard\u0026#34; does not exist. Installing it now. NAME: kubernetes-dashboard LAST DEPLOYED: Thu May 15 04:25:22 2025 NAMESPACE: kubernetes-dashboard STATUS: deployed REVISION: 1 TEST SUITE: None NOTES: ************************************************************************************************* *** PLEASE BE PATIENT: Kubernetes Dashboard may need a few minutes to get up and become ready *** ************************************************************************************************* Congratulations! You have just installed Kubernetes Dashboard in your cluster. To access Dashboard run: kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443 NOTE: In case port-forward command does not work, make sure that kong service name is correct. Check the services in Kubernetes Dashboard namespace using: kubectl -n kubernetes-dashboard get svc Dashboard will be available at: https://localhost:8443 查看服务是否已启动 1 2 3 4 5 6 7 8 9 10 11 watch kubectl get pods -n kubernetes-dashboard # 等待所有服务都为Running状态 # 可以使用下面的命令查看具体的情况，注意观察Events:中d # kubectl describe pods kubernetes-dashboard-api-7bcbc9b5bd-l76bc -n kubernetes-dashboard # ImagePullBackOff 错误后，重新启动步骤如下 # 1、将副本数缩放到 0 kubectl scale deployment kubernetes-dashboard-auth --replicas=0 -n kubernetes-dashboard # 2、将副本数恢复到 1 kubectl scale deployment kubernetes-dashboard-auth --replicas=1 -n kubernetes-dashboard 浏览器localhost访问 1 2 3 4 5 6 [root@k8s-master01 ~/dashboard]# kubectl -n kubernetes-dashboard port-forward svc/kubernetes-dashboard-kong-proxy 8443:443 Forwarding from 127.0.0.1:8443 -\u0026gt; 8443 Forwarding from [::1]:8443 -\u0026gt; 8443 Handling connection for 8443 Handling connection for 8443 E0515 07:19:42.893297 73613 portforward.go:409] an error occurred forwarding 8443 -\u0026gt; 8443: error forwarding port 8443 to pod 161802690a0f8d6f42e5761323b2aafa46ff37baf8eba8e44675e0ef760f08b7, uid : unable to do port forwarding: socat not found 以上日志说明环境缺少 socat ，安装之\n1 2 sudo apt-get update sudo apt-get install socat 登录界面\n查看kubernetes-dashboard下的账户\n1 kubectl -n kubernetes-dashboard get serviceaccounts 创建账户\n1 2 [root@k8s-master01 ~]# kubectl -n kubernetes-dashboard create serviceaccount kubernetes-dashboard serviceaccount/kubernetes-dashboard created 绑定角色权限\n1 2 3 kubectl create clusterrolebinding kubernetes-dashboard-admin \\ --clusterrole=cluster-admin \\ --serviceaccount=kubernetes-dashboard:kubernetes-dashboard 生成 Token\n1 kubectl -n kubernetes-dashboard create token kubernetes-dashboard 通过token成功访问\n浏览器节点访问 检查 NodePort 服务配置\n确保 kubernetes-dashboard 服务已正确配置为 NodePort，并且端口范围在 30000-32767 之间。\n检查服务配置\n运行以下命令，查看服务的配置：\n1 kubectl -n kubernetes-dashboard get svc kubernetes-dashboard-kong-proxy -o yaml 确认以下字段：\nspec.type 应为 NodePort。 spec.ports[0].nodePort 应为 32000。 如果配置不正确，可以通过以下命令更新：\n1 kubectl -n kubernetes-dashboard patch svc kubernetes-dashboard-kong-proxy -p \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;type\u0026#34;:\u0026#34;NodePort\u0026#34;,\u0026#34;ports\u0026#34;:[{\u0026#34;port\u0026#34;:443,\u0026#34;targetPort\u0026#34;:8443,\u0026#34;nodePort\u0026#34;:32000}]}}\u0026#39; 获取节点 IP 运行以下命令，获取节点的 IP 地址：\n1 2 kubectl get nodes -o wide # 使用 EXTERNAL-IP 或 INTERNAL-IP 访问 Dashboard。 生成 Token\n1 kubectl -n kubernetes-dashboard create token kubernetes-dashboard 通过token成功访问\n安装metrics-server 使用metrics-server实现主机资源监控，它可以解决的问题：解决上图中CPU和内存使用情况的获取\n资源参考链接：Releases · kubernetes-sigs/metrics-server\n通过配置文件进行安装 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 # 下载资源安装配置文件 wget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.7.2/components.yaml # 因为在使用 metrics-server 过程中没有创建证书，所以添加非安全的tls配置 spec: containers: - args: - --cert-dir=/tmp - --secure-port=10250 - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname - --kubelet-use-node-status-port - --metric-resolution=15s - --kubelet-insecure-tls # 这一行为添加内容 # 安装之 [root@k8s-master01 ~/dashboard]# kubectl apply -f components.yaml serviceaccount/metrics-server created clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created clusterrole.rbac.authorization.k8s.io/system:metrics-server created rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created service/metrics-server created deployment.apps/metrics-server created apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created 验证及授权 1 2 3 4 5 6 7 8 9 10 11 12 [root@k8s-master01 ~/dashboard]# kubectl get pods -n kube-system NAME READY STATUS RESTARTS AGE coredns-5dd5756b68-gbgsh 1/1 Running 12 (4h11m ago) 3d8h coredns-5dd5756b68-pm85d 1/1 Running 12 (4h11m ago) 3d8h etcd-k8s-master01 1/1 Running 12 (4h11m ago) 3d8h kube-apiserver-k8s-master01 1/1 Running 13 (4h11m ago) 3d8h kube-controller-manager-k8s-master01 1/1 Running 12 (4h11m ago) 3d8h kube-proxy-h7s9b 1/1 Running 6 (4h11m ago) 3d kube-proxy-qt8px 1/1 Running 6 (4h11m ago) 3d kube-proxy-wlvrg 1/1 Running 6 (4h11m ago) 3d kube-scheduler-k8s-master01 1/1 Running 12 (4h11m ago) 3d8h metrics-server-596474b58-697f8 1/1 Running 0 102s 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [root@k8s-master01 ~/dashboard]# kubectl top pods -n kube-system NAME CPU(cores) MEMORY(bytes) coredns-5dd5756b68-gbgsh 2m 22Mi coredns-5dd5756b68-pm85d 2m 51Mi etcd-k8s-master01 19m 128Mi kube-apiserver-k8s-master01 42m 532Mi kube-controller-manager-k8s-master01 11m 115Mi kube-proxy-h7s9b 1m 59Mi kube-proxy-qt8px 1m 50Mi kube-proxy-wlvrg 1m 52Mi kube-scheduler-k8s-master01 3m 49Mi metrics-server-596474b58-697f8 2m 22Mi [root@k8s-master01 ~/dashboard]# kubectl top nodes -n kube-system NAME CPU(cores) CPU% MEMORY(bytes) MEMORY% k8s-master01 131m 3% 4385Mi 86% k8s-worker01 41m 1% 2149Mi 76% k8s-worker02 45m 1% 2241Mi 79% ","date":"2024-07-22T00:00:00Z","image":"https://caijemmy.github.io/p/1.1-kubernetes%E9%9B%86%E7%BE%A4ui%E5%8F%8A%E4%B8%BB%E6%9C%BA%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7/cover_hu_e95a4276bf860a84.jpg","permalink":"https://caijemmy.github.io/p/1.1-kubernetes%E9%9B%86%E7%BE%A4ui%E5%8F%8A%E4%B8%BB%E6%9C%BA%E8%B5%84%E6%BA%90%E7%9B%91%E6%8E%A7/","title":"1.1 Kubernetes集群UI及主机资源监控"},{"content":"什么是 Keepalived？ 官网地址：https://keepalived.org/\nKeepalived 是一个基于 VRRP（Virtual Router Redundancy Protocol，虚拟路由器冗余协议） 的高可用性解决方案，主要用于实现 IP 故障转移 和 负载均衡。它通过将多个服务器组成一个虚拟路由器组，确保在主服务器故障时，备用服务器可以接管其 IP 地址和服务，从而实现高可用性。\nKeepalived 的核心功能 IP 故障转移（Failover）：\n当主服务器故障时，备用服务器会自动接管其 IP 地址，确保服务不中断。 健康检查（Health Check）：\n支持对后端服务（如 Nginx、Apache、MySQL 等）进行健康检查，确保只有健康的服务器才会接管 IP 地址。 负载均衡：\n支持基于 Layer 4（传输层）的负载均衡，可以将请求分发到多个后端服务器。 Keepalived 的工作原理 VRRP 协议：\nKeepalived 使用 VRRP 协议在多个服务器之间同步状态。 每个服务器在 VRRP 组中有一个优先级（Priority），优先级最高的服务器成为主服务器（Master），其他服务器成为备用服务器（Backup）。 主服务器负责持有虚拟 IP 地址（VIP），备用服务器处于待命状态。 故障检测：\nKeepalived 通过健康检查机制检测主服务器的状态。 如果主服务器故障，备用服务器中优先级最高的服务器会接管 VIP，成为新的主服务器。 状态切换：\n当主服务器恢复后，可以根据配置决定是否重新接管 VIP。 Keepalived 的典型应用场景 高可用性集群：\n用于实现 Web 服务器（如 Nginx、Apache）、数据库（如 MySQL）、负载均衡器（如 HAProxy）等的高可用性。 负载均衡：\n通过 Keepalived 和 IPVS（IP Virtual Server）实现基于 Layer 4 的负载均衡。 网络冗余：\n用于确保关键网络服务（如网关、防火墙）的高可用性。 Keepalived 的安装与配置 1. 安装 Keepalived 在基于 Debian/Ubuntu 的系统上：\n1 2 sudo apt update sudo apt install keepalived 在基于 CentOS/RHEL 的系统上：\n1 sudo yum install keepalived 2. 配置 Keepalived Keepalived 的配置文件通常位于 /etc/keepalived/keepalived.conf。\n示例配置：高可用性 Web 服务器 假设有两台服务器：\n主服务器：192.168.1.101 备用服务器：192.168.1.102 虚拟 IP 地址（VIP）：192.168.1.100 主服务器配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 vrrp_instance VI_1 { state MASTER # 主服务器 interface eth0 # 使用的网卡 virtual_router_id 51 # VRRP 组 ID，范围 0-255 priority 100 # 优先级，主服务器优先级最高 advert_int 1 # VRRP 广播间隔（秒） authentication { auth_type PASS # 认证类型 auth_pass 1234 # 认证密码 } virtual_ipaddress { 192.168.1.100 # 虚拟 IP 地址 } } 备用服务器配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 vrrp_instance VI_1 { state BACKUP # 备用服务器 interface eth0 # 使用的网卡 virtual_router_id 51 # VRRP 组 ID，与主服务器一致 priority 90 # 优先级，低于主服务器 advert_int 1 # VRRP 广播间隔（秒） authentication { auth_type PASS # 认证类型 auth_pass 1234 # 认证密码 } virtual_ipaddress { 192.168.1.100 # 虚拟 IP 地址 } } 3. 启动 Keepalived 在主服务器和备用服务器上启动 Keepalived：\n1 2 sudo systemctl start keepalived sudo systemctl enable keepalived 4. 测试故障转移 在主服务器上停止 Keepalived：\n1 sudo systemctl stop keepalived 检查备用服务器是否接管了 VIP：\n1 ip addr show eth0 恢复主服务器，检查 VIP 是否重新接管。\nKeepalived 的健康检查 Keepalived 支持对后端服务进行健康检查，确保只有健康的服务器才会接管 VIP。\n示例：Nginx 健康检查 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 vrrp_script chk_nginx { script \u0026#34;/usr/bin/pgrep nginx\u0026#34; # 检查 Nginx 是否运行 interval 2 # 检查间隔（秒） weight 2 # 优先级增减值 } vrrp_instance VI_1 { state MASTER interface eth0 virtual_router_id 51 priority 100 advert_int 1 authentication { auth_type PASS auth_pass 1234 } virtual_ipaddress { 192.168.1.100 } track_script { chk_nginx # 引用健康检查脚本 } } Keepalived 的日志 Keepalived 的日志默认输出到 /var/log/syslog（Debian/Ubuntu）或 /var/log/messages（CentOS/RHEL）。可以通过以下命令查看日志：\n1 tail -f /var/log/syslog | grep keepalived ","date":"2024-06-24T00:00:00Z","image":"https://caijemmy.github.io/p/1.0-keepalived/cover_hu_2d7c5f646cf5bd.jpg","permalink":"https://caijemmy.github.io/p/1.0-keepalived/","title":"1.0 keepalived"},{"content":"lua入门 官网：https://www.lua.org\nLua 是一种轻量级、高效、可嵌入的脚本语言\n教程参考：https://www.runoob.com/lua/lua-tutorial.html\n安装lua Linux 环境已安装 gcc、make。其他环境安装可以参考：https://www.runoob.com/lua/lua-environment.html 1 2 3 4 5 6 7 8 9 10 11 12 13 14 root@debian:~# curl -L -R -O https://www.lua.org/ftp/lua-5.4.7.tar.gz root@debian:~# tar zxf lua-5.4.7.tar.gz root@debian:~# cd lua-5.4.7/ root@debian:~/lua-5.4.7# make all test root@debian:~/lua-5.4.7# make install cd src \u0026amp;\u0026amp; mkdir -p /usr/local/bin /usr/local/include /usr/local/lib /usr/local/man/man1 /usr/local/share/lua/5.4 /usr/local/lib/lua/5.4 cd src \u0026amp;\u0026amp; install -p -m 0755 lua luac /usr/local/bin cd src \u0026amp;\u0026amp; install -p -m 0644 lua.h luaconf.h lualib.h lauxlib.h lua.hpp /usr/local/include cd src \u0026amp;\u0026amp; install -p -m 0644 liblua.a /usr/local/lib cd doc \u0026amp;\u0026amp; install -p -m 0644 lua.1 luac.1 /usr/local/man/man1 root@debian:~# echo \u0026#39;print(\u0026#34;Hello World!\u0026#34;)\u0026#39; \u0026gt; HelloWorld.lua root@debian:~# lua HelloWorld.lua Hello World! 基本语法 交互式 1 2 3 4 5 6 root@debian:~# lua -i Lua 5.4.7 Copyright (C) 1994-2024 Lua.org, PUC-Rio \u0026gt; print(\u0026#34;hello lua\u0026#34;) hello lua \u0026gt; os.exit() # 退出交互式 root@debian:~# 脚本式 1 2 3 4 root@debian:~# cat HelloWorld.lua print(\u0026#34;Hello World!\u0026#34;) root@debian:~# lua ./HelloWorld.lua Hello World! 注释 单行 1 2 3 4 5 root@debian:~# cat HelloWorld.lua -- 这是注释 print(\u0026#34;Hello World!\u0026#34;) root@debian:~# lua ./HelloWorld.lua Hello World! 多行 1 2 3 4 5 6 7 root@debian:~# cat HelloWorld.lua --[[ 这是注释 这是多行注释 --]] print(\u0026#34;Hello World!\u0026#34;) root@debian:~# lua ./HelloWorld.lua Hello World! 多行注释不支持嵌套 标识符 Lua 标识符用于定义一个变量，函数获取其他用户定义的项。标识符以一个字母 A 到 Z 或 a 到 z 或下划线 _ 开头后加上 0 个或多个字母，下划线，数字（0 到 9）。\n最好不要使用下划线加大写字母的标识符，因为Lua的保留字也是这样的。\nLua 不允许使用特殊字符如 @, $, 和 % 来定义标识符。 Lua 是一个区分大小写的编程语言。\n关键词 以下列出了 Lua 的保留关键词。保留关键字不能作为常量或变量或其他用户自定义标示符：\nand break do else elseif end false for function if in local nil not or repeat return then true until while goto 一般约定，以下划线开头连接一串大写字母的名字（比如 _VERSION）被保留用于 Lua 内部全局变量。\n全局变量 在默认情况下，变量总是认为是全局的。\n全局变量不需要声明，给一个变量赋值后即创建了这个全局变量，访问一个没有初始化的全局变量也不会出错，只不过得到的结果是：nil。\n如果你想删除一个全局变量，只需要将变量赋值为nil。换句话说, 当且仅当一个变量不等于nil时，这个变量即存在。\n数据类型 Lua 是动态类型语言，变量不要类型定义,只需要为变量赋值。 值可以存储在变量中，作为参数传递或结果返回。\nLua 中有 8 个基本类型分别为：nil、boolean、number、string、userdata、function、thread 和 table。\n数据类型 描述 nil 这个最简单，只有值nil属于该类，表示一个无效值（在条件表达式中相当于false）。 boolean 包含两个值：false和true。 number 表示双精度类型的实浮点数 string 字符串由一对双引号或单引号来表示 function 由 C 或 Lua 编写的函数 userdata 表示任意存储在变量中的C数据结构 thread 表示执行的独立线路，用于执行协同程序 table Lua 中的表（table）其实是一个\u0026quot;关联数组\u0026quot;（associative arrays），数组的索引可以是数字、字符串或表类型。在 Lua 里，table 的创建是通过\u0026quot;构造表达式\u0026quot;来完成，最简单构造表达式是{}，用来创建一个空表。 nil（空） nil 类型表示一种没有任何有效值，它只有一个值 \u0026ndash; nil，例如打印一个没有赋值的变量，便会输出一个 nil 值 对于全局变量和 table，nil 还有一个\u0026quot;删除\u0026quot;作用，给全局变量或者 table 表里的变量赋一个 nil 值，等同于把它们删掉 nil 作比较时应该加上双引号 \u0026quot; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 -- 对保留字nil进行type运算，结果返回nil字符串 print(type(nil)) print(type(type(nil))) -- 未定义变量返回的也是nil字符串 print(type(a)) print(type(type(a))) -- 对于全局变量和 table，nil 起\u0026#34;删除\u0026#34;作用 xx = \u0026#39;oo\u0026#39; print(xx) xx = nil print(xx) tab1 = { key1 = \u0026#34;val1\u0026#34;, key2 = \u0026#34;val2\u0026#34; } for k, v in pairs(tab1) do print(k .. \u0026#34; - \u0026#34; .. v) end tab1.key1 = nil for k, v in pairs(tab1) do print(k .. \u0026#34; - \u0026#34; .. v) end -- nil 作比较时应该加上双引号 \u0026#34; print(type(X)==nil) print(type(X)==\u0026#34;nil\u0026#34;) 1 2 3 4 5 6 7 8 9 10 11 12 13 # 运行结果 root@debian:/opt/lua# lua ./lua_type/test_nil.lua nil string nil string oo nil key1 - val1 key2 - val2 key2 - val2 false true boolean（布尔） boolean 类型只有两个可选值：true（真） 和 false（假），Lua 把 false 和 nil 看作是 false，其他的都为 true，数字 0 也是 true\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 print(type(true)) print(type(false)) print(type(nil)) if false or nil then print(\u0026#34;至少有一个是 true\u0026#34;) else print(\u0026#34;false 和 nil 都为 false\u0026#34;) end if 0 then print(\u0026#34;数字 0 是 true\u0026#34;) else print(\u0026#34;数字 0 为 false\u0026#34;) end 1 2 3 4 5 6 root@debian:/opt/lua# lua ./lua_type/test_boolean.lua boolean boolean nil false 和 nil 都为 false 数字 0 是 true number（数字） Lua 默认只有一种 number 类型 \u0026ndash; double（双精度）类型（默认类型可以修改 luaconf.h 里的定义），以下几种写法都被看作是 number 类型\n1 2 3 4 5 6 print(type(2)) print(type(2.2)) print(type(0.2)) print(type(2e+1)) print(type(0.2e-1)) print(type(7.8263692594256e-06)) 1 2 3 4 5 6 7 root@debian:/opt/lua# lua ./lua_type/test_number.lua number number number number number number string（字符串） 字符串由一对双引号或单引号来表示。 也可以用 2 个方括号 \u0026ldquo;[[]]\u0026rdquo; 来表示\u0026quot;一块\u0026quot;字符串，不支持嵌套。 在对一个数字字符串上进行算术操作时，Lua 会尝试将这个数字字符串转成一个数字，数字字符串中有非数字时，报错。 使用 # 来计算字符串的长度，放在字符串前面，1汉字为3个长度。 字符串连接使用的是 .. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 -- 字符串中如果需要单引号或双引号，需根据字符串使用情况进行转义 string1 = \u0026#34;this is\u0026#39; \u0026#39;\\\u0026#34;string1\u0026#34; string2 = \u0026#39;this is \u0026#34;string2\\\u0026#39;\u0026#34;\u0026#39; print(string1) print(string2) -- 块字符串不支持嵌套 html = [[ 这是块字符串 块字符串不支持嵌套 ]] print(html) -- 在对一个数字字符串上进行算术操作时，Lua 会尝试将这个数字字符串转成一个数字 print(\u0026#34;2\u0026#34; + 6) print(\u0026#34;2\u0026#34; + \u0026#34;6\u0026#34;) print(\u0026#34;2 + 6\u0026#34;) print(\u0026#34;-2e2\u0026#34; * \u0026#34;6\u0026#34;) -- print(\u0026#34;error\u0026#34; + 1) -- attempt to add a \u0026#39;string\u0026#39; with a \u0026#39;number\u0026#39; print(\u0026#34;2.1\u0026#34; + 1) -- 字符串连接 print(\u0026#34;a\u0026#34; .. \u0026#39;b\u0026#39;) print(157 .. 428) -- 计算字符串的长度 len_test = \u0026#34;你好 世界\u0026#34; print(#len_test) print(#\u0026#34;你好 lua\u0026#34;) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 root@debian:/opt/lua# lua ./lua_type/test_string.lua this is\u0026#39; \u0026#39;\u0026#34;string1 this is \u0026#34;string2\u0026#39;\u0026#34; 这是块字符串 块字符串不支持嵌套 8 8 2 + 6 -1200.0 3.1 ab 157428 13 10 table（表） 在 Lua 里，table 的创建是通过\u0026quot;构造表达式\u0026quot;来完成，最简单构造表达式是{}，用来创建一个空表。也可以在表里添加一些数据，直接初始化表。\nLua 中的表（table）其实是一个\u0026quot;关联数组\u0026quot;（associative arrays），数组的索引可以是数字或者是字符串。\n1 2 3 4 5 6 7 8 9 10 11 -- 创建一个空的 table local tbl1 = {} for k, v in pairs(tbl1) do print(k .. \u0026#34; : \u0026#34; .. v) end -- 直接初始表 local tbl2 = {\u0026#34;apple\u0026#34;, \u0026#34;pear\u0026#34;, \u0026#34;orange\u0026#34;, \u0026#34;grape\u0026#34;} for k, v in pairs(tbl2) do print(k .. \u0026#34; : \u0026#34; .. v) end 1 2 3 4 5 6 # 运行结果 root@debian:/opt/lua# lua ./lua_type/test_table.lua 1 : apple 2 : pear 3 : orange 4 : grape 不同于其他语言的数组把 0 作为数组的初始索引，在 Lua 里表的默认初始索引一般以 1 开始。 table 不会固定长度大小，有新数据添加时 table 长度会自动增长，没初始的 table 都是 nil。 function（函数） 在 Lua 中，函数是被看作是\u0026quot;第一类值（First-Class Value）\u0026quot;，函数可以存在变量里:\n1 2 3 4 5 6 7 8 9 10 function factorial1(n) if n == 0 then return 1 else return n * factorial1(n - 1) end end print(factorial1(5)) factorial2 = factorial1 print(factorial2(5)) 1 2 3 root@debian:/opt/lua# lua ./lua_type/test_func.lua 120 120 匿名函数（anonymous function） 1 2 3 4 5 6 7 8 9 10 11 12 13 function testFun(tab, fun) for k, v in pairs(tab) do print(fun(k, v)); end end tab = { key1 = \u0026#34;val1\u0026#34;, key2 = \u0026#34;val2\u0026#34; }; testFun(tab, function(key, val) --匿名函数 return key .. \u0026#34;=\u0026#34; .. val; end ); 1 2 3 4 5 root@debian:/opt/lua# lua ./lua_type/test_func.lua 120 120 key2=val2 key1=val1 thread（线程） 在 Lua 里，最主要的线程是协同程序（coroutine）。它跟线程（thread）差不多，拥有自己独立的栈、局部变量和指令指针，可以跟其他协同程序共享全局变量和其他大部分东西。\n线程跟协程的区别：线程可以同时多个运行，而协程任意时刻只能运行一个，并且处于运行状态的协程只有被挂起（suspend）时才会暂停。\nuserdata（自定义类型） userdata 是一种用户自定义数据，用于表示一种由应用程序或 C/C++ 语言库所创建的类型，可以将任意 C/C++ 的任意数据类型的数据（通常是 struct 和 指针）存储到 Lua 变量中调用。\n","date":"2024-06-24T00:00:00Z","image":"https://caijemmy.github.io/p/1.0-lua/cover_hu_1a65aef5375c430f.jpg","permalink":"https://caijemmy.github.io/p/1.0-lua/","title":"1.0 lua"}]